[{"id":0,"href":"/docs/people/temp/","title":" ","section":"People","content":" Jaeho Lee # Assistant Professor @ POSTECH EE (22.03\u0026ndash;)\nPrincipal Investigator @ EffL (22.03\u0026ndash;)\nVisiting Researcher @ Google (23.09\u0026ndash;)\nwebpage, mail, twitter\nJiwoon Lee # Efficient ML in the Wild üêä\nGraduate Student @ POSTECH EE (22.03\u0026ndash;)\nOffice Team Lead @ EffL\nKeywords: Model Merging, Federated Learning, Knowledge Distillation\nwebpage, mail\nJunwon Seo # Blazing-Fast Neural Field Generation üî•\nGraduate Student @ POSTECH EE (22.03\u0026ndash;)\nInfrastructure Team Lead @ EffL\nKeywords: Neural Field, Training Efficiency, Implicit Bias of SGD\nwebpage, mail\nSeungwoo Son # Compress Gigantic Transformers, but Efficiently ü§ë\nGraduate Student @ POSTECH EE (22.03\u0026ndash;)\nStudent Intern @ Google\nKeywords: Large Language Models, Masked Modeling, Knowledge Distillation\nwebpage, mail\nHagyeong Lee # Data Compression, but for more than what we see üîÆ\nGraduate Student @ POSTECH EE (22.09\u0026ndash;)\nAcademic Team Lead @ EffL\nKeywords: Data Compression, Model Bias, Visual-Language Model\nwebpage, mail, twitter\nMinkyu Kim # Harnessing Language Models for Multimodal Tasks üéôÔ∏è\nGraduate Student @ POSTECH AI (23.03\u0026ndash;)\nKeywords: Prefix Tuning, Multimodal Learning, Data Compression\nwebpage, mail, blog, recent project\nYu Ji Byun # Completing High-Resolution Videos with Low Resources üìπ\nGraduate Student @ POSTECH Defense Science (23.03\u0026ndash;)\nCaptain @ ROKMC\nKeywords: High-res Videos, Image Inpainting, Computer Vision\nmail, webpage\nJuyun Wee # Adaptive Processing of Extremely Long Data Sequences ‚è≥\nGraduate Student @ POSTECH EE (23.09\u0026ndash;)\nKeywords: Time-Series Forecasting, Test-Time Training, Self-supervised Learning\nwebpage, mail\nSeung-Ah Song # Administrative Staff @ EffL (22.03\u0026ndash;)\nmail\n"},{"id":1,"href":"/docs/research/focus/","title":"Focus","section":"Research","content":"The long-term goal of EffL is to make AI more responsible\u0026ndash;accessible, sustainable, and righteous.\nAs the first step, we are focusing on various facets of Efficient ML, which could help us make AI equally accessible to anybody on Earth, with no extreme carbon emission.\nIn particular, we work on three dimensions:\nInference. We develop fast, low-resource methods to serve massive multimodal AI, e.g., Model Compression Parallel Decoding Batch Scheduling Training. We resolve the compute / memory bottlenecks for training large-scale models, e.g., Meta-Learning Model Merging Parameter-Efficient Fine-Tuning Data Dimension. We design algorithms to handle data with extremely high dimensionality, e.g., Data Compression High-res Video Processing Time-Series Forecasting For more, see our papers.\n"},{"id":2,"href":"/docs/how-to-join/graduate/","title":"M.S./Ph.D. üéì","section":"How to Join?","content":"We have 2\u0026ndash;3 openings, through POSTECH EE and Graduate School of AI.\n(as of October 2023)\nPlease email Jaeho with your\nTranscript (with rank) CV/R√©sum√© A short statement of your research interests We will have a short coffee chat to discuss the next steps.\nHere are some notes by Jaeho.\nAdvising Statement # As an advisor, I am commited to these principles:\nkeep the group handleable-sized, have individual weekly meetings (30\u0026ndash;60mins at least), be maximally supportive to the career decisions of the students, and keep non-academic burdens minimal. I expect EffLers to\npatiently lead their own research agenda, maintain the highest level of academic honesty and intellectual humility, gain top-notch expertise in their own area, and willingly share their knowledge with fellow EffLers. Resources \u0026amp; Financial Support # We provide the followings for any M.S. or Ph.D. candidates.\nA competitive monthly stipend. This, of course, comes with research duties on the funded projects. A laptop/desktop computer, among some options. I strongly recommend getting a Linux/Mac. Any purchased computer is technically a university property; use for research only. Minimum 4 GPUs + Cluster access per person for research uses. We have a mixture of RTX 6000 Ada / RTX 4090 / A6000 / A5000. More GPUs on Clouds will be available, if needed. Travel fund, if you have a first-authored paper accepted to target ML conferences. Exclusive for Ph.D. candidates:\nAdditional support for computers \u0026amp; monitors. One-time travel grant for an ML conference, even without a paper accepted. "},{"id":3,"href":"/docs/people/members/","title":"Members","section":"People","content":" Jaeho Lee # Assistant Professor @ POSTECH EE (22.03\u0026ndash;)\nPrincipal Investigator @ EffL (22.03\u0026ndash;)\nVisiting Researcher @ Google (23.09\u0026ndash;)\nwebpage, mail, twitter\nJiwoon Lee # Efficient ML in the Wild üêä\nGraduate Student @ POSTECH EE (22.03\u0026ndash;)\nOffice Team Lead @ EffL\nKeywords: Model Merging, Federated Learning, Knowledge Distillation\nwebpage, mail\nJunwon Seo # Blazing-Fast Neural Field Generation üî•\nGraduate Student @ POSTECH EE (22.03\u0026ndash;)\nInfrastructure Team Lead @ EffL\nKeywords: Neural Field, Training Efficiency, Implicit Bias of SGD\nwebpage, mail\nSeungwoo Son # Compress Gigantic Transformers, but Efficiently ü§ë\nGraduate Student @ POSTECH EE (22.03\u0026ndash;)\nStudent Intern @ Google\nKeywords: Large Language Models, Masked Modeling, Knowledge Distillation\nwebpage, mail\nHagyeong Lee # Data Compression, but for more than what we see üîÆ\nGraduate Student @ POSTECH EE (22.09\u0026ndash;)\nAcademic Team Lead @ EffL\nKeywords: Data Compression, Model Bias, Visual-Language Model\nwebpage, mail, twitter\nMinkyu Kim # Harnessing Language Models for Multimodal Tasks üéôÔ∏è\nGraduate Student @ POSTECH AI (23.03\u0026ndash;)\nKeywords: Prefix Tuning, Multimodal Learning, Data Compression\nwebpage, mail, blog, recent project\nYu Ji Byun # Completing High-Resolution Videos with Low Resources üìπ\nGraduate Student @ POSTECH Defense Science (23.03\u0026ndash;)\nCaptain @ ROKMC\nKeywords: High-res Videos, Image Inpainting, Computer Vision\nmail, webpage\nJuyun Wee # Adaptive Processing of Extremely Long Data Sequences ‚è≥\nGraduate Student @ POSTECH EE (23.09\u0026ndash;)\nKeywords: Time-Series Forecasting, Test-Time Training, Self-supervised Learning\nwebpage, mail\nSeung-Ah Song # Administrative Staff @ EffL (22.03\u0026ndash;)\nmail\n"},{"id":4,"href":"/docs/people/intern/","title":"Interns","section":"People","content":" Interns # Sangyoon Lee (Summer \u0026lsquo;23\u0026ndash;)\nFast Neural Field Generation\nJiyun Bae (Summer \u0026lsquo;23\u0026ndash;)\nVisual Prompt Tuning\nSangbeom Ha (Summer \u0026lsquo;23\u0026ndash;)\nLarge-Scale Transformer Quantization\nMinyoung Kang (Fall \u0026lsquo;23\u0026ndash;)\nNeural Cellular Automata\nYousung Roh (Fall \u0026lsquo;23\u0026ndash;)\nByte-Processing Neural Networks\n"},{"id":5,"href":"/docs/how-to-join/interns/","title":"Interns üê•","section":"How to Join?","content":"We always welcome interns to our group.\nWhat will you do? # See this note for a brief description of what interns will be doing.\nHow to Apply? # Send an email to Jaeho, with a brief description of your:\nEducational background, e.g., the courses you took. Research interests and experiences (if any). "},{"id":6,"href":"/docs/people/past/","title":"Alumni","section":"People","content":" Past Interns # Jegwang Ryu (Summer \u0026lsquo;23)\nTest-time Training with Masked Modeling\nDohyun Kim (Summer \u0026lsquo;23)\nZero-th Order Optimization\nJuyun Wee (Spring \u0026lsquo;23 ‚Üí EffL)\nTime-Series Modeling with Transformers\nSoochang Song (Winter \u0026lsquo;22 \u0026ndash; Spring \u0026lsquo;23)\nModel Interpolation with SIRENs\nJeonghun Cho (Winter \u0026lsquo;22)\nPruning Models under Challenging Scenarios\nSeyeon Park (Winter \u0026lsquo;21 ‚Üí Yonsei)\nEfficient Attentions for Language Models\nHagyeong Lee (Winter \u0026lsquo;21 ‚Üí EffL)\nData Compression with Implicit Neural Representations\n"},{"id":7,"href":"/docs/how-to-join/intern_program/","title":" ","section":"How to Join?","content":" Internship Program # We treat interns differently with respect to your skill levels.\n(1) Level System # We\u0026rsquo;ll put you into one of three groups: L1, L2, L3, depending on your level of expertise.\nL1. You know very little about ML/DL. Goal: Complete an online DL coursework (e.g., this, this or this). L2. You are familiar with DL, but not research. Goal: Select a research topic and learn how to read papers. L3. You can read papers, and code in PyTorch/Jax. Goal: Read papers up to the research frontier, and start doing research. (2) Weekly Meeting # You will have a weekly meeting with Jaeho and other interns.\nIn every other meeting (i.e., twice a month), you are expected to do the following:\nL1. Cover a deep learning video lecture; provide your own version of summary. L2. Summarize a research paper, that is aligned with your interest. L3. Present your research progress. (3) Promotion # If you are L1 or L2, and feel you\u0026rsquo;re ready to get promoted, apply for a defense.\nIn the very next meeting, we will test if you have what it takes to become the next level.\n(4) Small Perks # Your own desk, located in the graduate student office. Free use of capsule coffee machines, printers, and Wi-Fi. Non-mandatory invitations to group lunches and dinners. "},{"id":8,"href":"/docs/people/member/hagyeong/","title":" ","section":"People","content":" Hagyeong Lee # (say something)\n"},{"id":9,"href":"/docs/people/member/jiwoon/","title":" ","section":"People","content":" Jiwoon Lee # üîç Research Interests # Model merging, Federated learning, Knowledge distillation\nüè´ Education # Pohang University of Science and Technology (POSTECH) (2015.03 ~ 2022.02) B.S. in Electrical Engineering -(2018.10 ~ 2020.05) Military service (2022.02 ~ Present) M.S. candidate in Electrical Engineering üè¢ Experience # (2017.06 ~ 2017.07) Research Intern, Preceding Patterning Team, SK Hynix (2018.06 ~ 2018.08) Research Intern, Embedded System Architecture Lab, POSTECH üìö Publication # Jiwoon Lee, Jaeho Lee, \u0026ldquo;Debiased Distillation by Transplanting the Last Layer\u0026rdquo;, ArXiv preprint, 2023. arxiv - Preliminary version presented in Workshop on Image Processing and Image Understanding 2023. üìû Contact # mail : jwlee9702@postech.ac.kr, easy9702@gmail.com Social accounts : linkedin\n"},{"id":10,"href":"/docs/people/member/junwon/","title":" ","section":"People","content":" Junwon Seo # üîç Research Interests # Neural Field, Training Efficiency, Implicit Bias of SGD\nüè´ Education # (2018.03 ~ 2022.02) B.S. in Computer Science and Engineering, Chung-Ang University\n(2022.02 ~ Present) M.S. candidate in Electrical Engineering, POSTECH\nüè¢ Experience # (2020.06 ~ 2021.09) Research Intern, Data Intelligence Lab, Chung-Ang University üìû Contact # mail : junwon.seo@postech.ac.kr, junwon.seo97@gmail.com\nSocial accounts : linkedin\n"},{"id":11,"href":"/docs/people/member/juyun/","title":" ","section":"People","content":" Juyun Wee # üîç Research Interests # Test-Time Training, Time-Series Forecasting\nüè´ Education # Pohang University of Science and Technology (POSTECH) (2018.03 ~ 2023.08) B.S. in Electrical Engineering (2023.09 ~ Present) M.S. candidate in Electrical Engineering University of Stuttgart, Germany (2022.08 ~ 2023.01) Exchange Student in Electrical Enginerring üè¢ Experience # (2023.03 ~ 2017.07) Research Intern, Efficient Learning Laboratory, POSTECH (2021.09 ~ 2017.01) Research Intern, Advanced Information Systems Laboratory, POSTECH (2020.06 ~ 2020.08) Startup Intern, R\u0026amp;D Team, Reziena (2019.06 ~ 2019.08) Startup Intern, R\u0026amp;D Team, Dodo tDo üìû Contact # mail : jywee@postech.ac.kr\n"},{"id":12,"href":"/docs/people/member/minkyu/","title":" ","section":"People","content":" Minkyu Kim # Contact # mail : minkyu.kim@postech.ac.kr, minkyu4506@gmail.com github : https://github.com/MinkyuKim26 Social accounts : linkedin\nPublication # Minkyu Kim*, Kim Sung-Bin*, Tae-Hyun Oh, \u0026ldquo;PREFIX TUNING FOR AUTOMATED AUDIO CAPTIONING\u0026rdquo;, ICASSP, 2023, Kim Sung-Bin*, Tae-Hyun Oh, \u0026ldquo;PREFIX TUNING FOR AUTOMATED AUDIO CAPTIONING\u0026rdquo;, ICASSP, 2023 [project page, code] ‚û°Ô∏è Selected as Oral presentation \u0026amp; Media coverage: covered by Yonhap News, SBS, and many local media\n"},{"id":13,"href":"/docs/people/member/seungwoo/","title":" ","section":"People","content":" Seungwoo Son # WIP\n"},{"id":14,"href":"/docs/people/member/yuji/","title":" ","section":"People","content":" Yuji Byun # üîç Research Interests # Image Inpainting, Computer Vision\nüè´ Education # (2011.02 ~ 2015.02) B.S. in Computer Science at Naval Academy (2023.02 ~ Present) M.S. candidate in Defense Science at POSTECH ü™ñ Experience # (2015. 03 ~ Present) ROKMC communication officer üìû Contact # mail : yujibyun@postech.ac.kr\n"},{"id":15,"href":"/docs/research/papers/","title":"Papers","section":"Research","content":"We mainly target top-tier ML conferences, e.g., NeurIPS / ICML / ICLR.\nSometimes, we also submit to domain-specific venues that involve ML, e.g., Vision/Language/Speech.\n2023 # Learning Large-scale Neural Fields via Context Pruned Meta-learning\nJihoon Tack, Subin Kim, Sihyun Yu, Jaeho Lee, Jinwoo Shin, and Jonathan R. Schwarz\nNeurIPS 2023 (ICLR 2023 Workshop: Neural Fields across Fields)\nBreaking the Spurious Causality of Conditional Generation via Fairness Intervention with Corrective Sampling\nJunhyun Nam, Sangwoo Mo, Jaeho Lee, and Jinwoo Shin\nTMLR 2023 (ICML 2023 Workshop: Spurious Correlations, Invariance, and Stability)\nModality-Agnostic Variational Compression of Implicit Neural Representations\nJonathan R. Schwarz, Jihoon Tack, Yee Whye Teh, Jaeho Lee, and Jinwoo Shin\nICML 2023 (ICLR 2023 Workshop: Neural Fields across Fields)\nBias-to-Text: Debiasing Unknown Visual Biases through Language Interpretation\nYounghyun Kim, Sangwoo Mo, Minkyu Kim, Kyungmin Lee, Jaeho Lee, and Jinwoo Shin\nICML 2023 Workshop: Spurious Correlations, Invariance, and Stability\nOn the Effectiveness of Sharpness-aware Minimization with Large Mini-batches\nJinseok Chung, Seonghwan Park, Jaeho Lee, and Namhoon Lee\nICML 2023 Workshop: High-Dimensional Learning Dynamics\nMaskedKD: Efficient Distillation of Vision Transformers with Masked Images\nSeungwoo Son, Namhoon Lee, and Jaeho Lee\nICLR 2023 Workshop: Sparsity in Neural Networks (IPIU 2023 Oral ü•â)\nPrefix Tuning for Automated Audio Captioning\nMinkyu Kim, Kim Sung-Bin, and Tae-Hyun Oh\nICASSP 2023 Oral\nCommunication-Efficient Split Learning via Adaptive Feature-wise Compression\nYongjeong Oh, Jaeho Lee, Christopher G. Brinton, and Yo-Seb Jeon\nUnder Review\nIn Search of a Data Transformation that Accelerates Neural Field Training\nJunwon Seo, Sangyoon Lee, and Jaeho Lee\nUnder Review\nSemi-Ensemble: A Simple Approach to Over-Parameterize Model Interpolation\nJiwoon Lee and Jaeho Lee\nUnder Review\nDebiased Distillation by Transplanting the Last Layer\nJiwoon Lee and Jaeho Lee\narXiv preprint 2302.11187 (IPIU 2023)\n2022 # Scalable Neural Video Representations with Learnable Positional Features\nSubin Kim, Sihyun Yu, Jaeho Lee, and Jinwoo Shin\nNeurIPS 2022 (project page)\nMeta-learning with Self-improving Momentum Targets\nJihoon Tack, Jongjin Park, Hankook Lee, Jaeho Lee and Jinwoo Shin\nNeurIPS 2022\nSpread Spurious Attribute: Improving Worst-Group Accuracy with Spurious Attribute Estimation\nJunhyun Nam, Jaehyung Kim, Jaeho Lee, and Jinwoo Shin\nICLR 2022\nZero-shot Blind Image Denoising via Implicit Neural Representations\nChaewon Kim, Jaeho Lee, and Jinwoo Shin\narXiv preprint 2204.02405\n2021 # Meta-learning Sparse Implicit Neural Representations\nJaeho Lee, Jihoon Tack, Namhoon Lee, and Jinwoo Shin\nNeurIPS 2021\nCo2L: Contrastive Continual Learning\nHyuntak Cha, Jaeho Lee, and Jinwoo Shin\nICCV 2021\nProvable Memorization via Deep Neural Networks using Sub-linear Parameters\nSejun Park, Jaeho Lee, Chulhee Yun, and Jinwoo Shin\nCOLT 2021 (DeepMath 2020 Oral)\nMinimum Width for Universal Approximation\nSejun Park, Chulhee Yun, Jaeho Lee, and Jinwoo Shin\nICLR 2021 Spotlight (DeepMath 2020 Oral)\nLayer-adaptive Sparsity for the Magnitude-based Pruning\nJaeho Lee, Sejun Park, Sangwoo Mo, Sungsoo Ahn, and Jinwoo Shin\nICLR 2021\nMASKER: Masked Keyword Regularization for Reliable Text Generation\nSeung Jun Moon, Sangwoo Mo, Kimin Lee, Jaeho Lee, and Jinwoo Shin\nAAAI 2021\nGreedyprune: Layer-wise Optimization Algorithms for Magnitude-based Pruning\nVinoth Nandakumar and Jaeho Lee\nSparse Neural Network Workshop 2021\n2020 # Learning Bounds for Risk-sensitive Learning\nJaeho Lee, Sejun Park, and Jinwoo Shin\nNeurIPS 2020\nLearning from Failure: Training Debiased Classifier from Biased Classifier\nJunhyun Nam, Hyuntak Cha, Sungsoo Ahn, Jaeho Lee, and Jinwoo Shin\nNeurIPS 2020\nLookahead: A Far-sighted Alternative of Magnitude-based Pruning\nSejun Park, Jaeho Lee, Sangwoo Mo, and Jinwoo Shin\nICLR 2020\nPre-2020 # Learning Finite-dimensional Coding Schemes with Nonlinear Reconstruction Maps\nJaeho Lee and Maxim Raginsky\nSIMODS 2019\nMinimax Statistical Learning with Wasserstein Distances\nJaeho Lee and Maxim Raginsky\nNeurIPS 2018 Spotlight\nOn MMSE Estimation from Quantized Observations in the Nonasymptotic Regime\nJaeho Lee, Maxim Raginsky, and Pierre Moulin\nISIT 2015\nDomestic Posters # An Empirical Study on the Bias of Generative Image Compression\nHagyeong Lee and Jaeho Lee\nIPIU 2023\nIs Sparse Identification Model Sufficiently Biased?\nJunwon Seo and Jaeho Lee\nIPIU 2023\n"}]