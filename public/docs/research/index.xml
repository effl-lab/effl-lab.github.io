<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>EffL @ POSTECH</title>
    <link>https://effl.postech.ac.kr/docs/research/</link>
    <description>Recent content on EffL @ POSTECH</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language><atom:link href="https://effl.postech.ac.kr/docs/research/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Focus</title>
      <link>https://effl.postech.ac.kr/docs/research/focus/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://effl.postech.ac.kr/docs/research/focus/</guid>
      <description>The long-term goal of EffL is to make AI more responsible&amp;ndash;accessible, sustainable, and righteous.
As the first step, we are focusing on various facets of Efficient ML, which could help us make AI equally accessible to anybody on Earth, with no extreme carbon emission.
In particular, we work on three dimensions:
Inference. We develop fast, low-resource methods to serve massive multimodal AI, e.g., Model Compression Parallel Decoding Batch Scheduling Training. We resolve the compute / memory bottlenecks for training large-scale models, e.</description>
    </item>
    
    <item>
      <title>Papers</title>
      <link>https://effl.postech.ac.kr/docs/research/papers/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://effl.postech.ac.kr/docs/research/papers/</guid>
      <description>We mainly target top-tier ML conferences, e.g., NeurIPS / ICML / ICLR.
Sometimes, we also submit to domain-specific venues that involve ML, e.g., Vision/Language/Speech.
2023 # Semi-Ensemble: A Simple Approach to Over-Parameterize Model Interpolation
Jiwoon Lee and Jaeho Lee
NeurIPS 2023 Workshop: Unifying Representations in Neural Models
In Search of a Data Transformation that Accelerates Neural Field Training
Junwon Seo, Sangyoon Lee, and Jaeho Lee
NeurIPS 2023 Workshop: Attributing Model Behavior at Scale</description>
    </item>
    
  </channel>
</rss>
