[{"id":0,"href":"/docs/people/temp/","title":" ","section":"People","content":" Jaeho Lee # Assistant Professor @ POSTECH EE (22.03\u0026ndash;)\nPrincipal Investigator @ EffL (22.03\u0026ndash;)\nVisiting Researcher @ Google (23.09\u0026ndash;)\nwebpage, mail, twitter\nJiwoon Lee # Efficient ML in the Wild 🐊\nGraduate Student @ POSTECH EE (22.03\u0026ndash;)\nOffice Team Lead @ EffL\nKeywords: Model Merging, Federated Learning, Knowledge Distillation\nwebpage, mail\nJunwon Seo # Blazing-Fast Neural Field Generation 🔥\nGraduate Student @ POSTECH EE (22.03\u0026ndash;)\nInfrastructure Team Lead @ EffL\nKeywords: Neural Field, Training Efficiency, Implicit Bias of SGD\nwebpage, mail\nSeungwoo Son # Compress Gigantic Transformers, but Efficiently 🤑\nGraduate Student @ POSTECH EE (22.03\u0026ndash;)\nStudent Intern @ Google\nKeywords: Large Language Models, Masked Modeling, Knowledge Distillation\nwebpage, mail\nHagyeong Lee # Data Compression, but for more than what we see 🔮\nGraduate Student @ POSTECH EE (22.09\u0026ndash;)\nAcademic Team Lead @ EffL\nKeywords: Data Compression, Model Bias, Visual-Language Model\nwebpage, mail, twitter\nMinkyu Kim # Harnessing Language Models for Multimodal Tasks 🎙️\nGraduate Student @ POSTECH AI (23.03\u0026ndash;)\nKeywords: Prefix Tuning, Multimodal Learning, Data Compression\nwebpage, mail, blog, recent project\nYu Ji Byun # Completing High-Resolution Videos with Low Resources 📹\nGraduate Student @ POSTECH Defense Science (23.03\u0026ndash;)\nCaptain @ ROKMC\nKeywords: High-res Videos, Image Inpainting, Computer Vision\nmail, webpage\nJuyun Wee # Adaptive Processing of Extremely Long Data Sequences ⏳\nGraduate Student @ POSTECH EE (23.09\u0026ndash;)\nKeywords: Time-Series Forecasting, Test-Time Training, Self-supervised Learning\nwebpage, mail\nSeung-Ah Song # Administrative Staff @ EffL (22.03\u0026ndash;)\nmail\n"},{"id":1,"href":"/docs/research/focus/","title":"Focus","section":"Research","content":"The long-term goal of EffL is to make AI more responsible\u0026ndash;accessible, sustainable, and righteous.\nAs the first step, we are focusing on various facets of Efficient ML, which could help us make AI equally accessible to anybody on Earth, with no extreme carbon emission.\nIn particular, we work on three dimensions:\nInference. We develop fast, low-resource methods to serve massive multimodal AI, e.g., Model Compression Parallel Decoding Batch Scheduling Training. We resolve the compute / memory bottlenecks for training large-scale models, e.g., Meta-Learning Model Merging Parameter-Efficient Fine-Tuning Data Dimension. We design algorithms to handle data with extremely high dimensionality, e.g., Data Compression High-res Video Processing Time-Series Forecasting For more, see our papers.\n"},{"id":2,"href":"/docs/how-to-join/graduate/","title":"M.S./Ph.D. 🎓","section":"How to Join?","content":"Please email Jaeho with your\nTranscript (with rank) CV/Résumé A short statement of your research interests We will have a short coffee chat to discuss the next steps.\nHere are some notes by Jaeho.\nAdvising Statement # As an advisor, I am commited to these principles:\nkeep the group handleable-sized, have individual weekly meetings (30\u0026ndash;60mins at least), be maximally supportive to the career decisions of the students, and keep non-academic burdens minimal. I expect EffLers to\npatiently lead their own research agenda, maintain the highest level of academic honesty and intellectual humility, gain top-notch expertise in their own area, and willingly share their knowledge with fellow EffLers. Resources \u0026amp; Financial Support # We provide the followings for any M.S. or Ph.D. candidates.\nA competitive monthly stipend. This, of course, comes with research duties on the funded projects. A laptop/desktop computer, among some options. I strongly recommend getting a Linux/Mac. Any purchased computer is technically a university property; use for research only. Minimum 4 GPUs + Cluster access per person for research uses. We have a mixture of RTX 6000 Ada / RTX 4090 / A6000 / A5000. More GPUs on Clouds will be available, if needed. Travel fund, if you have a first-authored paper accepted to target ML conferences. Exclusive for Ph.D. candidates:\nAdditional support for computers \u0026amp; monitors. One-time travel grant for an ML conference, even without a paper accepted. "},{"id":3,"href":"/docs/people/members/","title":"Members","section":"People","content":" Jaeho Lee # Assistant Professor @ POSTECH EE (22.03\u0026ndash;)\nPrincipal Investigator @ EffL (22.03\u0026ndash;)\nVisiting Researcher @ Google (23.09\u0026ndash;)\nwebpage, mail, twitter\nSeungwoo Son # Compress Gigantic Transformers, but Efficiently 🤑\nGraduate Student @ POSTECH EE (22.03\u0026ndash;)\nStudent Intern @ Google\nKeywords: Large Language Models, Model Compression\nwebpage, mail\nHagyeong Lee # Data Compression, but for more than what we see 🔮\nGraduate Student @ POSTECH EE (22.09\u0026ndash;)\nKeywords: Data Compression, Model Bias, Visual-Language Model\nwebpage, mail, twitter\nMinkyu Kim # Harnessing Language Models for Multimodal Tasks 🎙️\nGraduate Student @ POSTECH AI (23.03\u0026ndash;)\nInfrastructure Team Lead @ EffL\nKeywords: Prefix Tuning, Multimodal Learning, Data Compression\nwebpage, mail, blog, recent project, twitter\nYu Ji Byun # Completing High-Resolution Videos with Low Resources 📹\nGraduate Student @ POSTECH Defense Science (23.03\u0026ndash;)\nCaptain @ ROKMC\nKeywords: High-res Videos, Image Inpainting, Computer Vision\nmail, webpage\nJuyun Wee # Adaptive Processing of Extremely Long Data Sequences Graduate Student @ POSTECH EE (23.09\u0026ndash;)\nOffice Team Lead @ EffL\nKeywords: Time-Series Forecasting, Test-Time Training, Self-supervised Learning\nwebpage, mail\nMinjae Park # Accelerating Large-Scale Models for Videos 🎥\nGraduate Student @ POSTECH EE (24.02\u0026ndash;)\nKeywords: State-Space Models, Video Processing, Model Compression\nSangyoon Lee # Hyperefficient Neural Fields ⚡️\nGraduate Student @ POSTECH AI (24.02\u0026ndash;)\nKeywords: Fast Neural Fields, Editable Neural Fields, Neural Compression\nJiyun Bae # Data-adaptive Visual Prompts 🕶️\nGraduate Student @ POSTECH AI (24.02\u0026ndash;)\nAcademic Team Lead @ EffL\nKeywords: Visual Prompts, Black-box models, LLMs, Fairness\nHyunjong Ok # Advancing Audio Technologies for Multimodal Intelligence 🔈\nGraduate Student @ POSTECH AI (24.02\u0026ndash;) Keywords: (Too many!)\nSeung-Ah Song # Administrative Staff @ EffL (22.03\u0026ndash;)\nmail\n"},{"id":4,"href":"/docs/people/intern/","title":"Interns","section":"People","content":" Interns # Sangbeom Ha (Summer \u0026lsquo;23\u0026ndash;)\nLarge-Scale Model Quantization\nInkwan Hwang (Fall \u0026lsquo;23\u0026ndash;)\nLarge-Scale Model Pruning\nTaesun Yeom (Winter \u0026lsquo;23\u0026ndash;)\nTraining and Inference Efficiency for Neural Fields\nMinhee Lee (Winter \u0026lsquo;23\u0026ndash;)\nSpeculative Decoding\nJeonghyun Choi (Winter \u0026lsquo;23\u0026ndash;)\nProperties of Data Augmentation\nJegwang Ryu (Summer \u0026lsquo;23 \u0026ndash;\u0026gt; Samsung \u0026ndash;\u0026gt; Winter \u0026lsquo;23\u0026ndash;)\nAccelerated Training by Masking\nSeunghyun Kim (Spring \u0026lsquo;24\u0026ndash;)\nLifelong training of Foundation Models\nWonjun Cho (Spring \u0026lsquo;24\u0026ndash;)\n(TBD)\n"},{"id":5,"href":"/docs/how-to-join/interns/","title":"Interns 🐥","section":"How to Join?","content":"We always welcome interns to our group.\nWhat will you do? # See this note for a brief description of what interns will be doing.\nHow to Apply? # Send an email to Jaeho, with a brief description of your:\nEducational background, e.g., the courses you took. Research interests and experiences (if any). "},{"id":6,"href":"/docs/people/past/","title":"Alumni","section":"People","content":" Past Graduate Students # Jiwoon Lee # Efficient ML in the Wild 🐊\nM.S. @ POSTECH EE (22.03\u0026ndash;24.02)\nEx-Office Team Lead @ EffL\nKeywords: Model Merging, Federated Learning, Knowledge Distillation\nwebpage, mail\nJunwon Seo # Blazing-Fast Neural Field Generation 🔥\nM.S. @ POSTECH EE (22.03\u0026ndash;24.02)\nEx-Infrastructure Team Lead @ EffL\nKeywords: Neural Field, Training Efficiency, Implicit Bias of SGD\nwebpage, mail\nPast Interns # Minjae Park (Winter \u0026lsquo;23; now at EffL)\nFaster State-Space Models\nMinyoung Kang (Fall \u0026lsquo;23\u0026ndash;Winter \u0026lsquo;23)\nNeural Cellular Automata\nYousung Roh (Fall \u0026lsquo;23\u0026ndash;Winter \u0026lsquo;23)\nByte-Processing Neural Networks\nJiyun Bae (Summer \u0026lsquo;23\u0026ndash;Fall \u0026lsquo;23; now at EffL)\nVisual Prompt Tuning\nSangyoon Lee (Summaer\u0026ndash;Fall \u0026lsquo;23; now at EffL)\nFast Neural Field Generation\nJegwang Ryu (Summer \u0026lsquo;23; now at Samsung)\nTest-time Training with Masked Modeling\nDohyun Kim (Summer \u0026lsquo;23; now at 🫡)\nZeroth Order Optimization\nJuyun Wee (Spring \u0026lsquo;23 → EffL)\nTime-Series Modeling with Transformers\nSoochang Song (Winter \u0026lsquo;22 \u0026ndash; Spring \u0026lsquo;23; now exchange student at 🇫🇷)\nModel Interpolation with SIRENs\nJeonghun Cho (Winter \u0026lsquo;22)\nPruning Models under Challenging Scenarios\nSeyeon Park (Winter \u0026lsquo;21 → Yonsei)\nEfficient Attentions for Language Models\nHagyeong Lee (Winter \u0026lsquo;21 → EffL)\nData Compression with Implicit Neural Representations\n"},{"id":7,"href":"/docs/onboarding/","title":"Onboarding","section":"Docs","content":"EffL에 오신 것을 환영합니다! 🥳🥳\nAbout this page # 이 페이지는 대학원 입학허가를 받으신 분들의 온보딩을 돕기 위해서 작성되었습니다.\n입학허가를 받은 날부터 포항에 도착하여 Slack Workspace 액세스를 받기 전까지의 step들을 다룹니다.\nSlack 초대를 받은 이후에는, 슬랙 내에 기재 예정인 온보딩 절차를 따르시면 됩니다.\n물론, 별도로 학과에서 요구하는 절차들도 따라주세요.\nStill not sure? EffL에 오실지, 다른 곳으로 진학하실지 아직 미정이라면 어떤 것도 진행하지 마시기 바랍니다. 결정 이후에 진행해주세요.\n현 버전은 pre-alpha version으로, 일부 정보가 누락됐을 수 있습니다 😅 Before Arrival # 1. GitHub Access PI에게 메일을 보내, 사용하시는 GitHub 계정을 알려주세요. 연구실 웹페이지 repository에 collaborator로 추가드릴 예정입니다.\nGit? Git/GitHub 이용에 익숙하시길 기대합니다 🥹 잘 모르신다면 웹에 튜토리얼이 많으니 공부해두세요 (e.g., 여기).\n2. Present yourself in EffL Webpage 연구실 홈페이지 내 서브페이지로 본인의 페이지를 만들어주세요. 만약 본인의 웹페이지가 이미 있는 경우, 링크만 해두셔도 좋습니다.\n현재 연구실 홈페이지는 Hugo를 사용하고 있으며, Markdown 문서를 작성하는 것 만으로도 개인 소개 서브페이지를 손쉽게 만들 수 있습니다. 사용법은 튜토리얼을 참고하세요.\n대략, 다음 순서를 따라가시면 됩니다.\nRepository의 content/docs/people/member/ 디렉토리에 자신의 이름으로 된 문서를 추가합니다 (e.g., jaeho.md) 초보자의 경우, 다른 분이 이미 작성하신 문서를 복붙하시면 좋습니다. 헤더에서 이름을 수정하고, 내용을 자신을 잘 소개할 수 있는 내용으로 자유롭게 수정해주세요. 이미지 등을 추가하고 싶으신 경우 assets/ 서브디렉토리를 만들어 저장하시면 될 것 같습니다. Repository의 content/docs/people/members.md에 자신의 이름을 추가하고, 웹페이지를 링크해주세요. 컴퓨터에 Hugo가 설치되어 있는 경우, 정상적으로 웹페이지가 작동하는 것을 확인한 뒤 (예: hugo server 커맨드 이후 localhost 접속) 컴파일하여 repository에 업데이트해주세요. 질문. 궁금한 사항이 있다거나, 실수로 웹페이지를 터뜨렸다면 PI에게 메일로 알려주세요.\n3. Purchase your Computers 도착하시자마자 일을 시작하실 수 있게 미리 연구용 컴퓨터를 구입하시기 바랍니다.\n예산. 2023년 10월 기준, 최대 300만원의 예산을 배정할 예정입니다.\n용도. 데스크탑/노트북/모니터 구입에 사용하실 수 있고, 키보드/마우스/모니터암 등은 구입이 불가능합니다.\n소유권. 학교 소유물이기 때문에 연구용으로만 사용해주시고, 졸업 시 반납하셔야 합니다. 개인용으로 사용하실 노트북은 따로 구비하시길 권합니다.\n추천. MacBook Pro 혹은 iMac을 구입하시는 것을 강력하게 추천드립니다. 딥러닝서버에서 사용하는 linux 문법에 익숙해지기 좋고, 터미널 사용이 편리하며, 보안과 시스템 안정성이 뛰어나기 때문입니다. 그리고 PI가 앱등이이기 때문에 co-work하기 편합니다 😅\n구입하고 싶으신 모델을 결정하고 나서, 견적서를 PI에게 전달해주세요. 연구비로 구입하는 것이기 때문에, 정확한 구입시기와 구매처는 변동이 있을 수 있습니다 (Apple의 경우 교육할인이 아쉽게도 불가능합니다).\n4. Plan for Pohang Life 포항의 거주옵션들을 알아보시고 합류일정을 PI에게 메일로 알려주세요.\n학기 시작 전 미리 합류하시는 경우에도 기숙사를 이용가능한 경우가 있습니다. 학과사무실을 통해 이용가능한 옵션을 문의해주세요.\n추천. 재택근무 경험이 많지 않으시다면, 기숙사 이용을 추천드립니다. 이외에도 유강, SK뷰, 효자시장 등에 괜찮은 housing option이 있다고 알고 있습니다. 연구실은 제2공학관에 위치해 있으니 위치 선정에 참고하시기 바랍니다. 네이버 지도\n5. Keep in Touch PI에게 주기적으로 연락을 취해 연구 및 관련 공부를 미리 진행하시면 대학원 생활에서 훨씬 더 많은 것을 얻어가실 수 있습니다.\n많은 경우, PI가 몇 가지 미션을 드릴 수 있을 것입니다\u0026hellip; 🤔\nAfter Arrival # 6. Let Jaeho Know PI에게 메일을 보내, 무사히 도착했다는 사실과 언제 연구실에 첫 방문을 하실 지 알려주세요.\n보통은 PI와 간단하게 미팅을 한 뒤, PI가 연구실로 모시고 가서 간단하게 소개를 드릴 예정입니다.\n위치. PI의 오피스는 제2공학관 407호입니다.\n7. Desk Assignment 연구실에 오셔서 자리를 고르세요. 빈 자리 중에서 마음에 드는 곳을 선택하시면 됩니다.\n(TODO: 누가 어디 앉는지 기록하는 시스템)\n위치. EffL Lab은 제2공학관 404호에 있습니다.\n재배정. 매년 가을학기 초, 첫번째 Group Meeting에서 재배정을 진행할 예정입니다.\n8. Request Slack Access 연구실 Slack이 있습니다. PI에게 메일을 보내, 어떤 이메일 계정으로 초대메일을 보내면 되는지 알려주세요.\n(TODO: Slack 학생 관리자)\n주의. 연구실 내 소통은 Slack을 이용해주시고, 불가피한 경우를 제외하고 전화/문자/카톡 등은 삼가주세요.\n9. After Getting Slack Access 이후의 step들은 Slack 내 온보딩 기능을 활용하여 안내를 할 예정입니다.\n이 기능은 아직까지는 구현되지 않았습니다 ㅠㅠ\nMeanwhile, Office Team Lead (이지운님)을 통해 필요한 것들을 여쭤보세요.\n(TODO: 프린터, 와이파이, 연구실 비밀번호, 서버 및 서버실 액세스, 포비스, MS Teams)\n"},{"id":8,"href":"/docs/how-to-join/intern_program/","title":"Internship Program","section":"How to Join?","content":" Internship Program # We treat interns differently with respect to your skill levels.\n(1) Level System # We\u0026rsquo;ll put you into one of three groups: L1, L2, L3, depending on your level of expertise.\nL1. You know very little about ML/DL. Goal: Complete an online DL coursework (e.g., this, this or this). L2. You are familiar with DL, but not research. Goal: Select a research topic and learn how to read papers. L3. You can read papers, and code in PyTorch/Jax. Goal: Read papers up to the research frontier, and start doing research. (2) Weekly Meeting # You will have a weekly meeting with Jaeho and other interns.\nIn every other meeting (i.e., twice a month), you are expected to do the following:\nL1. Cover a deep learning video lecture; provide your own version of summary. L2. Summarize a research paper, that is aligned with your interest. L3. Present your research progress. (3) Promotion # If you are L1 or L2, and feel you\u0026rsquo;re ready to get promoted, apply for a defense.\nIn the very next meeting, we will test if you have what it takes to become the next level.\n(4) Small Perks # Your own desk, located in the graduate student office. Free use of capsule coffee machines, printers, and Wi-Fi. Non-mandatory invitations to group lunches and dinners. "},{"id":9,"href":"/docs/people/member/hagyeong/","title":"Hagyeong Lee","section":"People","content":" Hagyeong Lee # (say something)\n"},{"id":10,"href":"/docs/people/member/jiwoon/","title":"Jiwoon Lee","section":"People","content":" Jiwoon Lee # 🔍 Research Interests # Model merging, Federated learning, Knowledge distillation\n🏫 Education # Pohang University of Science and Technology (POSTECH) (2015.03 ~ 2022.02) B.S. in Electrical Engineering -(2018.10 ~ 2020.05) Military service (2022.02 ~ Present) M.S. candidate in Electrical Engineering 🏢 Experience # (2017.06 ~ 2017.07) Research Intern, Preceding Patterning Team, SK Hynix (2018.06 ~ 2018.08) Research Intern, Embedded System Architecture Lab, POSTECH 📚 Publication # Jiwoon Lee, Jaeho Lee, \u0026ldquo;Debiased Distillation by Transplanting the Last Layer\u0026rdquo;, ArXiv preprint, 2023. arxiv - Preliminary version presented in Workshop on Image Processing and Image Understanding 2023. 📞 Contact # mail : jwlee9702@postech.ac.kr, easy9702@gmail.com Social accounts : linkedin\n"},{"id":11,"href":"/docs/people/member/junwon/","title":"Junwon Seo","section":"People","content":" Junwon Seo # 🔍 Research Interests # Neural Field, Training Efficiency, Implicit Bias of SGD\n🏫 Education # (2018.03 ~ 2022.02) B.S. in Computer Science and Engineering, Chung-Ang University\n(2022.02 ~ Present) M.S. candidate in Electrical Engineering, POSTECH\n🏢 Experience # (2020.06 ~ 2021.09) Research Intern, Data Intelligence Lab, Chung-Ang University 📞 Contact # mail : junwon.seo@postech.ac.kr, junwon.seo97@gmail.com\nSocial accounts : linkedin\n"},{"id":12,"href":"/docs/people/member/juyun/","title":"Juyun Wee","section":"People","content":" Juyun Wee # 🔍 Research Interests # Test-Time Training, Time-Series Forecasting\n🏫 Education # Pohang University of Science and Technology (POSTECH) (2018.03 ~ 2023.08) B.S. in Electrical Engineering (2023.09 ~ Present) M.S. candidate in Electrical Engineering University of Stuttgart, Germany (2022.08 ~ 2023.01) Exchange Student in Electrical Enginerring 🏢 Experience # (2023.03 ~ 2017.07) Research Intern, Efficient Learning Laboratory, POSTECH (2021.09 ~ 2017.01) Research Intern, Advanced Information Systems Laboratory, POSTECH (2020.06 ~ 2020.08) Startup Intern, R\u0026amp;D Team, Reziena (2019.06 ~ 2019.08) Startup Intern, R\u0026amp;D Team, Dodo tDo 📞 Contact # mail : jywee@postech.ac.kr\n"},{"id":13,"href":"/docs/people/member/minkyu/","title":"Minkyu Kim","section":"People","content":" Minkyu Kim # Hello! 👋\n🔍 Research Interests # Prefix Tuning, Image Compression, Multi-modal learning\n🏫 Education # Chung-Ang University (CAU)\n(2017.03 ~ 2023.02) B.S in Electrical \u0026amp; Electronics Engineering Pohang University of Science and Technology (POSTECH)\n(2023.02 ~ Present) M.S. candidate in Graduate School of AI 🏢 Experience # (2021.09 ~ 2021.12) Research Intern, Computer Vision Lab, Korea University (2022.07 ~ 2022.08) Undergraduate Research Fellowship, Algorithmic Machine Intelligence Lab, POSTECH 📜 Publication # Minkyu Kim*, Kim Sung-Bin*, Tae-Hyun Oh, \u0026ldquo;PREFIX TUNING FOR AUTOMATED AUDIO CAPTIONING\u0026rdquo;, ICASSP, 2023 [project page]\n➡️ Selected as Oral presentation \u0026amp; Media coverage: covered by Yonhap News, SBS, and many local media\n📝 Services # Peer review, IEEE/ACM Transactions on Audio, Speech, and Language Processing, 2023\n📞 Contact # mail : minkyu.kim@postech.ac.kr, minkyu4506@gmail.com github : https://github.com/MinkyuKim26 Social accounts : linkedin\n"},{"id":14,"href":"/docs/research/papers/","title":"Papers","section":"Research","content":"We mainly target top-tier ML conferences, e.g., NeurIPS / ICML / ICLR.\nSometimes, we also submit to domain-specific venues that involve ML, e.g., Vision/Language/Speech.\nNote: Some papers below does not involve the PI.\n2024 # In Search of a Data Transformation that Accelerates Neural Field Training\nJunwon Seo, Sangyoon Lee, Kwang In Kim, and Jaeho Lee\nCVPR 2024 (NeurIPS 2023 Workshop: Attributing Model Behavior at Scale)\nDiscovering and Mitigating Visual Biases through Keyword Explanation\nYounghyun Kim, Sangwoo Mo, Minkyu Kim, Kyungmin Lee, Jaeho Lee, and Jinwoo Shin\nCVPR 2024 (ICML 2023 Workshop: Spurious Correlations, Invariance, and Stability)\nHybrid Neural Represetntations for Spherical Data\nHyomin Kim, Yunhui Jang, Jaeho Lee, Sungsoo Ahn\narXiv preprint 2402.05965\n2023 # Semi-Ensemble: A Simple Approach to Over-Parameterize Model Interpolation\nJiwoon Lee and Jaeho Lee\nNeurIPS 2023 Workshop: Unifying Representations in Neural Models\nLearning Large-scale Neural Fields via Context Pruned Meta-learning\nJihoon Tack, Subin Kim, Sihyun Yu, Jaeho Lee, Jinwoo Shin, and Jonathan R. Schwarz\nNeurIPS 2023 (ICLR 2023 Workshop: Neural Fields across Fields)\nBreaking the Spurious Causality of Conditional Generation via Fairness Intervention with Corrective Sampling\nJunhyun Nam, Sangwoo Mo, Jaeho Lee, and Jinwoo Shin\nTMLR 2023 (ICML 2023 Workshop: Spurious Correlations, Invariance, and Stability)\nModality-Agnostic Variational Compression of Implicit Neural Representations\nJonathan R. Schwarz, Jihoon Tack, Yee Whye Teh, Jaeho Lee, and Jinwoo Shin\nICML 2023 (ICLR 2023 Workshop: Neural Fields across Fields)\nOn the Effectiveness of Sharpness-aware Minimization with Large Mini-batches\nJinseok Chung, Seonghwan Park, Jaeho Lee, and Namhoon Lee\nICML 2023 Workshop: High-Dimensional Learning Dynamics\nMaskedKD: Efficient Distillation of Vision Transformers with Masked Images\nSeungwoo Son, Namhoon Lee, and Jaeho Lee\nICLR 2023 Workshop: Sparsity in Neural Networks (IPIU 2023 Oral 🥉)\nPrefix Tuning for Automated Audio Captioning\nMinkyu Kim, Kim Sung-Bin, and Tae-Hyun Oh\nICASSP 2023 Oral\nCommunication-Efficient Split Learning via Adaptive Feature-wise Compression\nYongjeong Oh, Jaeho Lee, Christopher G. Brinton, and Yo-Seb Jeon\nUnder Review\nDebiased Distillation by Transplanting the Last Layer\nJiwoon Lee and Jaeho Lee\narXiv preprint 2302.11187 (IPIU 2023)\n2022 # Scalable Neural Video Representations with Learnable Positional Features\nSubin Kim, Sihyun Yu, Jaeho Lee, and Jinwoo Shin\nNeurIPS 2022 (project page)\nMeta-learning with Self-improving Momentum Targets\nJihoon Tack, Jongjin Park, Hankook Lee, Jaeho Lee and Jinwoo Shin\nNeurIPS 2022\nSpread Spurious Attribute: Improving Worst-Group Accuracy with Spurious Attribute Estimation\nJunhyun Nam, Jaehyung Kim, Jaeho Lee, and Jinwoo Shin\nICLR 2022\nZero-shot Blind Image Denoising via Implicit Neural Representations\nChaewon Kim, Jaeho Lee, and Jinwoo Shin\narXiv preprint 2204.02405\nFew-shot Unlearning\nYoungsik Yoon, Jinhwan Nam, Hyojeong Yun, Jaeho Lee, Dongwoo Kim, and Jungseul Ok\nUnder Review\n2021 # Meta-learning Sparse Implicit Neural Representations\nJaeho Lee, Jihoon Tack, Namhoon Lee, and Jinwoo Shin\nNeurIPS 2021\nCo2L: Contrastive Continual Learning\nHyuntak Cha, Jaeho Lee, and Jinwoo Shin\nICCV 2021\nProvable Memorization via Deep Neural Networks using Sub-linear Parameters\nSejun Park, Jaeho Lee, Chulhee Yun, and Jinwoo Shin\nCOLT 2021 (DeepMath 2020 Oral)\nMinimum Width for Universal Approximation\nSejun Park, Chulhee Yun, Jaeho Lee, and Jinwoo Shin\nICLR 2021 Spotlight (DeepMath 2020 Oral)\nLayer-adaptive Sparsity for the Magnitude-based Pruning\nJaeho Lee, Sejun Park, Sangwoo Mo, Sungsoo Ahn, and Jinwoo Shin\nICLR 2021\nMASKER: Masked Keyword Regularization for Reliable Text Generation\nSeung Jun Moon, Sangwoo Mo, Kimin Lee, Jaeho Lee, and Jinwoo Shin\nAAAI 2021\nGreedyprune: Layer-wise Optimization Algorithms for Magnitude-based Pruning\nVinoth Nandakumar and Jaeho Lee\nSparse Neural Network Workshop 2021\n2020 # Learning Bounds for Risk-sensitive Learning\nJaeho Lee, Sejun Park, and Jinwoo Shin\nNeurIPS 2020\nLearning from Failure: Training Debiased Classifier from Biased Classifier\nJunhyun Nam, Hyuntak Cha, Sungsoo Ahn, Jaeho Lee, and Jinwoo Shin\nNeurIPS 2020\nLookahead: A Far-sighted Alternative of Magnitude-based Pruning\nSejun Park, Jaeho Lee, Sangwoo Mo, and Jinwoo Shin\nICLR 2020\nPre-2020 # Learning Finite-dimensional Coding Schemes with Nonlinear Reconstruction Maps\nJaeho Lee and Maxim Raginsky\nSIMODS 2019\nMinimax Statistical Learning with Wasserstein Distances\nJaeho Lee and Maxim Raginsky\nNeurIPS 2018 Spotlight\nOn MMSE Estimation from Quantized Observations in the Nonasymptotic Regime\nJaeho Lee, Maxim Raginsky, and Pierre Moulin\nISIT 2015\nDomestic Posters # An Empirical Study on the Bias of Generative Image Compression\nHagyeong Lee and Jaeho Lee\nIPIU 2023\nIs Sparse Identification Model Sufficiently Biased?\nJunwon Seo and Jaeho Lee\nIPIU 2023\n"},{"id":15,"href":"/docs/people/member/seungwoo/","title":"Seungwoo Son","section":"People","content":" Seungwoo Son # test\n"},{"id":16,"href":"/docs/people/member/yuji/","title":"Yuji Byun","section":"People","content":" Yuji Byun # 🔍 Research Interests # Image Inpainting, Computer Vision\n🏫 Education # (2011.02 ~ 2015.02) B.S. in Computer Science at Naval Academy (2023.02 ~ Present) M.S. candidate in Defense Science at POSTECH 🪖 Experience # (2015. 03 ~ Present) ROKMC communication officer 📞 Contact # mail : yujibyun@postech.ac.kr\n"}]