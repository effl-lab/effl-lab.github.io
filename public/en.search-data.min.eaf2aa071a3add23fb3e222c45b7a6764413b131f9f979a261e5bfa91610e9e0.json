[{"id":0,"href":"/docs/people/temp/","title":" ","section":"People","content":" Jaeho Lee # Assistant Professor @ POSTECH EE (22.03\u0026ndash;)\nPrincipal Investigator @ EffL (22.03\u0026ndash;)\nVisiting Researcher @ Google (23.09\u0026ndash;)\nwebpage, mail, twitter\nJiwoon Lee # Efficient ML in the Wild ğŸŠ\nGraduate Student @ POSTECH EE (22.03\u0026ndash;)\nOffice Team Lead @ EffL\nKeywords: Model Merging, Federated Learning, Knowledge Distillation\nwebpage, mail\nJunwon Seo # Blazing-Fast Neural Field Generation ğŸ”¥\nGraduate Student @ POSTECH EE (22.03\u0026ndash;)\nInfrastructure Team Lead @ EffL\nKeywords: Neural Field, Training Efficiency, Implicit Bias of SGD\nwebpage, mail\nSeungwoo Son # Compress Gigantic Transformers, but Efficiently ğŸ¤‘\nGraduate Student @ POSTECH EE (22.03\u0026ndash;)\nStudent Intern @ Google\nKeywords: Large Language Models, Masked Modeling, Knowledge Distillation\nwebpage, mail\nHagyeong Lee # Data Compression, but for more than what we see ğŸ”®\nGraduate Student @ POSTECH EE (22.09\u0026ndash;)\nAcademic Team Lead @ EffL\nKeywords: Data Compression, Model Bias, Visual-Language Model\nwebpage, mail, twitter\nMinkyu Kim # Harnessing Language Models for Multimodal Tasks ğŸ™ï¸\nGraduate Student @ POSTECH AI (23.03\u0026ndash;)\nKeywords: Prefix Tuning, Multimodal Learning, Data Compression\nwebpage, mail, blog, recent project\nYu Ji Byun # Completing High-Resolution Videos with Low Resources ğŸ“¹\nGraduate Student @ POSTECH Defense Science (23.03\u0026ndash;)\nCaptain @ ROKMC\nKeywords: High-res Videos, Image Inpainting, Computer Vision\nmail, webpage\nJuyun Wee # Adaptive Processing of Extremely Long Data Sequences â³\nGraduate Student @ POSTECH EE (23.09\u0026ndash;)\nKeywords: Time-Series Forecasting, Test-Time Training, Self-supervised Learning\nwebpage, mail\nSeung-Ah Song # Administrative Staff @ EffL (22.03\u0026ndash;)\nmail\n"},{"id":1,"href":"/docs/research/focus/","title":"Focus","section":"Research","content":"Our long-term goal is to make AI more responsible\u0026ndash;accessible, sustainable, and righteous.\nAs the first step, we focus on various facets of Efficient ML, which helps us make AI equally accessible to everyone.\nIn particular, we work on three dimensions:\nInference. We develop fast, low-resource methods to serve massive multimodal AI, e.g., Model Compression Parallel Decoding Batch Scheduling Training. We resolve the compute / memory bottlenecks for training large-scale models, e.g., Meta-Learning Model Merging Parameter-Efficient Fine-Tuning Data Dimension. We design algorithms to handle data with extremely high dimensionality, e.g., Data Compression High-res Video Processing Time-Series Forecasting See our papers for more.\n"},{"id":2,"href":"/docs/How-to-Join/graduate/","title":"M.S./Ph.D. ğŸ“","section":"How to Join?","content":"Please email Jaeho with your\nTranscript (with rank) CV/RÃ©sumÃ© A short statement of your research interests We will have a short coffee chat to discuss the next steps.\nAdvising Statement # As an advisor, I am commited to these principles:\nkeep the group handleable-sized, have individual weekly meetings (30\u0026ndash;60mins at least), be maximally supportive to your career decisions, and keep non-academic burdens minimal. I expect EffLers to\npatiently lead their own research agenda, maintain the highest level of academic honesty and intellectual humility, gain top-notch expertise in their own area, and willingly share their knowledge with fellow EffLers. "},{"id":3,"href":"/docs/people/members/","title":"Members","section":"People","content":" Principal Investigator # Jaeho Lee Assistant Professor @ POSTECH EE (22.03--)\nPrincipal Investigator @ EffL (22.03--)\nVisiting Researcher @ Google (23.09--)\nwebpage, mail, twitter Ph.D. track # Sangyoon Lee Hyperefficient Neural Fields âš¡ï¸\nPOSTECH AI (24.02--)\nKeywords: Fast Neural Fields, Editable Neural Fields, Neural Compression\nwebpage, mail Minjae Park Accelerating Large-Scale Models for Videos ğŸ¥\nM.S.+Ph.D. @ POSTECH EE (24.02--)\nKeywords: State-Space Models, Model Compression\nwebpage, mail Hyunjong Ok Advancing Audio Technologies for Multimodal Intelligence ğŸ”ˆ\nPOSTECH AI (24.02--)\nKeywords: Efficient LLM, Multi-modal learning, Audio Technologies\nwebpage, mail Taesun Yeom Understanding Training Dynamics of Neural Fields\nM.S.+Ph.D. @ POSTECH AI (24.09--)\nKeywords: Neural Fields, Learning Dynamics\nGoogle Scholar M.S. track # Minkyu Kim Harnessing Language Models for Multimodal Tasks ğŸï¸ğŸ™ï¸\nPOSTECH AI (23.03--)\nKeywords: Multimodal learning, Image processing, Audio captioning\nwebpage, mail, blog, recent project, twitter Yu Ji Byun Completing High-Resolution Videos with Low Resources ğŸ“¹\nPOSTECH Defense Science (23.03--)\nCaptain @ ROKMC\nKeywords: High-res Videos, Image Inpainting, Computer Vision\nwebpage, mail Juyun Wee Compressing Large Language Models ğŸ€\nPOSTECH EE (23.09--)\nOffice Team Lead @ EffL\nKeywords: LLM Compression, Model Pruning, Model Efficiency\nwebpage, mail Jiyun Bae Data-dependent Visual Prompts ğŸ•¶ï¸\nPOSTECH AI (24.02--)\nAcademic Team Lead @ EffL\nKeywords: Visual Prompts, Black-box models, LLMs, Fairness\nwebpage, mail Jegwang Ryu Compression in the Neural Network Weight Space\nPOSTECH EE (24.09--)\nKeywords: Efficient LLM, Accelerated Training\nGoogle Scholar, mail Seunghyun Kim Low-precision Training of LLM\nPOSTECH EE (24.09--)\nKeywords: LLM, Quantization\nStaff # Seung-Ah Song # Administrative Staff @ EffL (22.03\u0026ndash;)\nmail\n"},{"id":4,"href":"/docs/people/intern/","title":"Interns","section":"People","content":" Current Interns # Deokyeong Lee (Fall \u0026lsquo;24\u0026ndash;)\nContrastive Decoding with Hallucination\nSuho Yoo (Summer \u0026lsquo;24\u0026ndash;)\nModel Compression\nJaejin Kim (Fall \u0026lsquo;24\u0026ndash;)\nSpeculative Decoding of LLMs\nTaehyeok Ha (Fall \u0026lsquo;24\u0026ndash;)\nEfficient Inference for LLM\nYoonjoo Nam (Summer \u0026lsquo;24\u0026ndash;)\nSafety \u0026amp; Robustness on Vision Tasks\nHyeontae Jeong (Fall \u0026lsquo;24\u0026ndash;)\nEfficient Learning of Language Models\nWaitlist (Winter \u0026lsquo;25\u0026ndash;) # Suho Yoo, Yoonjoo Nam, Taehyuk Ha, Gwanjoong Kim, Minjun Kang, Youna Shin, Deokyoung Lee, Hyeontae Jeong, Jinho Kim\n"},{"id":5,"href":"/docs/How-to-Join/interns/","title":"Interns ğŸ¥","section":"How to Join?","content":"We always welcome interns to our group.\nWhat will you do? # See this note for a brief description of what interns will be doing.\nHow to Apply? # Send an email to Jaeho, with a brief description of your:\nEducational background, e.g., the courses you took. Research interests and experiences (if any). "},{"id":6,"href":"/docs/research/papers/","title":"Papers","section":"Research","content":"We mainly target top-tier ML conferences, and other domain-specific venues that involve ML.\n2025 # Towards Federated Low-Rank Adaptation with Rank-Heterogeneous Communication\nYuji Byun and Jaeho Lee\nNAACL 2025 (NeurIPS 2024 AFM Workshop)\nFast Training of Sinusoidal Neural Fields via Scaling Initialization\nTaesun Yeom,* Sangyoon Lee,* and Jaeho Lee\nICLR 2025\nZIP: An Efficient Zeroth-order Prompt Tuning for Black-box Vision-Language Models\nSeonghwan Park, Jaehyeon Jeong, Yongjun Kim, Jaeho Lee, and Namhoon Lee\nICLR 2025\nAudioBERT: Audio Knowledge Augmented Language Model\nHyunjong Ok,* Suho Yoo,* and Jaeho Lee\nICASSP 2025 (Outstanding paper award ğŸ† @ JKAIA 2024)\ncode, dataset\nCommunication-Efficient Split Learning via Adaptive Feature-wise Compression\nYongjeong Oh, Jaeho Lee, Christopher G. Brinton, and Yo-Seb Jeon\nIEEE TNNLS\n2024 # Decoding with Limited Teacher Supervision Requires Understanding When to Trust the Teacher\nHyunjong Ok, Jegwang Ryu and Jaeho Lee\nEMNLP 2024\nPrefixing Attention Sinks can Mitigate Activation Outliers for Large Language Model Quantization\nSeungwoo Son, Wonpyo Park, Woohyun Han, Kyuyeun Kim, and Jaeho Lee\nEMNLP 2024\nRethinking Pruning Large Language Models: Benefits and Pitfalls of Reconstruction Error Minimization\nSungbin Shin, Wonpyo Park, Jaeho Lee, and Namhoon Lee\nEMNLP 2024\nThe Role of Masking for Efficient Supervised Knowledge Distillation of Vision Transformers\nSeungwoo Son, Jegwang Ryu, Namhoon Lee, and Jaeho Lee\nECCV 2024 (ICLR 2023 Sparsity Workshop, Outstanding paper award ğŸ¥‰ @ IPIU 2023)\nproject page, code\nNeural Image Compression with Text-guided Encoding for both Pixel-level and Perceptual Fidelity\nHagyeong Lee,* Minkyu Kim,* Jun-Hyuk Kim, Seungeon Kim, Dokwan Oh, and Jaeho Lee\nICML 2024\nproject page, code\nHybrid Neural Representations for Spherical Data\nHyomin Kim, Yunhui Jang, Jaeho Lee, and Sungsoo Ahn\nICML 2024\nIn Search of a Data Transformation that Accelerates Neural Field Training\nJunwon Seo,* Sangyoon Lee,* Kwang In Kim, and Jaeho Lee\nCVPR 2024 Oral (top 0.78%) (NeurIPS 2023 ATTRIB Workshop)\ncode demo\nDiscovering and Mitigating Visual Biases through Keyword Explanation\nYounghyun Kim,* Sangwoo Mo,* Minkyu Kim, Kyungmin Lee, Jaeho Lee, and Jinwoo Shin\nCVPR 2024 Highlight (top 2.8%) (ICML 2023 SCIS Workshop)\ncode\nSCANNER: Knowledge-Enhanced Approach for Robust Multi-modal Named Entity Recognition of Unseen Entities\nHyunjong Ok, Taeho Kil, Sukmin Seo, and Jaeho Lee\nNAACL 2024\nFew-shot Unlearning\nYoungsik Yoon, Jinhwan Nam, Hyojeong Yun, Jaeho Lee, Dongwoo Kim, and Jungseul Ok\nIEEE S\u0026amp;P 2024\nAttention-aware Semantic Communications for Collaborative Inference\nJiwoong Im,* Nayoung Kwon,* Taewoo Park, Jiheon Woo, Jaeho Lee, and Yongjune Kim\nIEEE IoT Journal (IEEE Communication Theory Workshop 2024)\nConstructing a Singing Style Captioning Dataset\nHyunjong Ok and Jaeho Lee\narXiv 2409.09866\ndataset\n2023 # Learning Large-scale Neural Fields via Context Pruned Meta-learning\nJihoon Tack, Subin Kim, Sihyun Yu, Jaeho Lee, Jinwoo Shin, and Jonathan R. Schwarz\nNeurIPS 2023 (ICLR 2023 Neural Fields Workshop)\ncode\nModality-Agnostic Variational Compression of Implicit Neural Representations\nJonathan R. Schwarz, Jihoon Tack, Yee Whye Teh, Jaeho Lee, and Jinwoo Shin\nICML 2023 (ICLR 2023 Neural Fields Workshop)\nBreaking the Spurious Causality of Conditional Generation via Fairness Intervention with Corrective Sampling\nJunhyun Nam, Sangwoo Mo, Jaeho Lee, and Jinwoo Shin\nTMLR 2023 (ICML 2023 SCIS Workshop)\nSemi-Ensemble: A Simple Approach to Over-Parameterize Model Interpolation\nJiwoon Lee and Jaeho Lee\nNeurIPS 2023 Workshop: UniReps\nOn the Effectiveness of Sharpness-aware Minimization with Large Mini-batches\nJinseok Chung, Seonghwan Park, Jaeho Lee, and Namhoon Lee\nICML 2023 Workshop: HDLD\nDebiased Distillation by Transplanting the Last Layer\nJiwoon Lee and Jaeho Lee\narXiv 2302.11187 (IPIU 2023)\n2022 # Scalable Neural Video Representations with Learnable Positional Features\nSubin Kim, Sihyun Yu, Jaeho Lee, and Jinwoo Shin\nNeurIPS 2022\nproject page\nMeta-learning with Self-improving Momentum Targets\nJihoon Tack, Jongjin Park, Hankook Lee, Jaeho Lee and Jinwoo Shin\nNeurIPS 2022\nSpread Spurious Attribute: Improving Worst-Group Accuracy with Spurious Attribute Estimation\nJunhyun Nam, Jaehyung Kim, Jaeho Lee, and Jinwoo Shin\nICLR 2022\nZero-shot Blind Image Denoising via Implicit Neural Representations\nChaewon Kim, Jaeho Lee, and Jinwoo Shin\narXiv 2204.02405\n2021 # Meta-learning Sparse Implicit Neural Representations\nJaeho Lee, Jihoon Tack, Namhoon Lee, and Jinwoo Shin\nNeurIPS 2021\nCo2L: Contrastive Continual Learning\nHyuntak Cha, Jaeho Lee, and Jinwoo Shin\nICCV 2021\nProvable Memorization via Deep Neural Networks using Sub-linear Parameters\nSejun Park, Jaeho Lee, Chulhee Yun, and Jinwoo Shin\nCOLT 2021 (DeepMath 2020 Oral)\nMinimum Width for Universal Approximation\nSejun Park, Chulhee Yun, Jaeho Lee, and Jinwoo Shin\nICLR 2021 Spotlight (DeepMath 2020 Oral)\nLayer-adaptive Sparsity for the Magnitude-based Pruning\nJaeho Lee, Sejun Park, Sangwoo Mo, Sungsoo Ahn, and Jinwoo Shin\nICLR 2021\nMASKER: Masked Keyword Regularization for Reliable Text Generation\nSeung Jun Moon, Sangwoo Mo, Kimin Lee, Jaeho Lee, and Jinwoo Shin\nAAAI 2021\nGreedyprune: Layer-wise Optimization Algorithms for Magnitude-based Pruning\nVinoth Nandakumar and Jaeho Lee\nSparse Neural Network Workshop 2021\n2020 # Learning Bounds for Risk-sensitive Learning\nJaeho Lee, Sejun Park, and Jinwoo Shin\nNeurIPS 2020\nLearning from Failure: Training Debiased Classifier from Biased Classifier\nJunhyun Nam, Hyuntak Cha, Sungsoo Ahn, Jaeho Lee, and Jinwoo Shin\nNeurIPS 2020\nLookahead: A Far-sighted Alternative of Magnitude-based Pruning\nSejun Park,* Jaeho Lee,* Sangwoo Mo, and Jinwoo Shin\nICLR 2020\nPre-2020 # Learning Finite-dimensional Coding Schemes with Nonlinear Reconstruction Maps\nJaeho Lee and Maxim Raginsky\nSIMODS 2019\nMinimax Statistical Learning with Wasserstein Distances\nJaeho Lee and Maxim Raginsky\nNeurIPS 2018 Spotlight\nOn MMSE Estimation from Quantized Observations in the Nonasymptotic Regime\nJaeho Lee, Maxim Raginsky, and Pierre Moulin\nISIT 2015\nDomestic Posters ğŸ¯ # An Empirical Study on the Bias of Generative Image Compression # Hagyeong Lee and Jaeho Lee\nIPIU 2023\nIs Sparse Identification Model Sufficiently Biased? # Junwon Seo and Jaeho Lee\nIPIU 2023\n"},{"id":7,"href":"/docs/people/past/","title":"Alumni","section":"People","content":" Alumni # Seungwoo Son M.S. @ POSTECH EE (22.03--24.08): \"The role of masking for efficient supervised knowledge distillation of vision transformers\"\nExpertise. Model compression of large transformers, using quantization and distillation\nNext step. Samsung Research\nlinkedin, mail Hagyeong Lee M.S. @ POSTECH EE (22.09--24.08): \"Neural image compression with text-guided encoding for both pixel-level and perceptual fidelity\"\nExpertise. Multimodal data compression\nNext step. KBSI (í•œêµ­ê¸°ì´ˆê³¼í•™ì§€ì›ì—°êµ¬ì›)\nwebpage, mail, twitter Jiwoon Lee M.S. @ POSTECH EE (22.03--24.02): \"Semi-ensemble: A simple approach to over-parameterized model interpolation\"\nExpertise. Model Merging, federated Learning, and knowledge distillation\nNext step. FASOO\nmail Junwon Seo M.S. @ POSTECH EE (22.03--24.02): \"In search of a data transformation that accelerates neural field training\"\nExpertise. Fast training of neural fields\nNext step. LivsMed (ì „ë¬¸ì—°êµ¬ìš”ì›)\nmail Past Interns # Seoyun Jeong (Summer \u0026lsquo;24)\nCollaborative Decoding with Compressed Models\nYewon Hwang (Summer \u0026lsquo;24)\nQuantizing Diffusion Model\nSangbeom Ha (Summer \u0026lsquo;23\u0026ndash;Spring \u0026lsquo;24)\nLarge-Scale Model Quantization\nInkwan Hwang (Fall \u0026lsquo;23\u0026ndash;Summer \u0026lsquo;24; now at ğŸ«¡)\nLarge-Scale Model Pruning\nwebpage\nTaesun Yeom (Winter \u0026lsquo;23\u0026ndash;Spring \u0026lsquo;24; Now at EffL)\nTraining and Inference Efficiency for Neural Fields\nMinhee Lee (Winter \u0026lsquo;23)\nSpeculative Decoding\nJegwang Ryu (Summer \u0026lsquo;23, Spring \u0026lsquo;24; Now at EffL)\nAccelerated Training by Masking\nSeunghyun Kim (Spring \u0026lsquo;24; Now at EffL)\nEfficient RAG LLM\nWonjun Cho (Spring \u0026lsquo;24)\nModel Compression\nSubeom Heo (Spring\u0026ndash;Summer \u0026lsquo;24)\nAccelerating Video Diffusion Models\nJeonghyun Choi (Winter \u0026lsquo;23)\nProperties of Data Augmentation\nMinjae Park (Winter \u0026lsquo;23; now at EffL)\nFaster State-Space Models\nMinyoung Kang (Fall\u0026ndash;Winter \u0026lsquo;23)\nNeural Cellular Automata\nYousung Roh (Fall \u0026lsquo;23\u0026ndash;Winter \u0026lsquo;23)\nByte-Processing Neural Networks\nJiyun Bae (Summer\u0026ndash;Fall \u0026lsquo;23; now at EffL)\nVisual Prompt Tuning\nSangyoon Lee (Summaer\u0026ndash;Fall \u0026lsquo;23; now at EffL)\nFast Neural Field Generation\nDohyun Kim (Summer \u0026lsquo;23; now at ğŸ«¡)\nZeroth Order Optimization\nJuyun Wee (Spring \u0026lsquo;23 â†’ EffL)\nTime-Series Modeling with Transformers\nSoochang Song (Winter \u0026lsquo;22 \u0026ndash; Spring \u0026lsquo;23; now exchange student at ğŸ‡«ğŸ‡·)\nModel Interpolation with SIRENs\nJeonghun Cho (Winter \u0026lsquo;22)\nPruning Models under Challenging Scenarios\nSeyeon Park (Winter \u0026lsquo;21 â†’ Yonsei)\nEfficient Attentions for Language Models\nHagyeong Lee (Winter \u0026lsquo;21 â†’ EffL)\nData Compression with Implicit Neural Representations\n"},{"id":8,"href":"/docs/research/collaborators/","title":"Collaborators","section":"Research","content":"Our recent industry collaborators include:\nSamsung Advanced Institute of Technology: ICML 2024, TMLR 2023 Google: EMNLP 2024; as a visiting faculty researcher, and as a student intern DeepMind: ICML 2023 NAVER: NAACL 2024 Krafton: CVPR 2024 VPIX medical: (Private project) In addition, we have worked with universities abroad:\nUniversity College London: NeurIPS 2023, ICML 2023 University of Michigan: CVPR 2024 "},{"id":9,"href":"/docs/onboarding/","title":"Onboarding","section":"Docs","content":"EffLì— ì˜¤ì‹  ê²ƒì„ í™˜ì˜í•©ë‹ˆë‹¤! ğŸ¥³ğŸ¥³\nAbout this page # ì´ í˜ì´ì§€ëŠ” ëŒ€í•™ì› ì…í•™í—ˆê°€ë¥¼ ë°›ìœ¼ì‹  ë¶„ë“¤ì˜ ì˜¨ë³´ë”©ì„ ë•ê¸° ìœ„í•´ì„œ ì‘ì„±ë˜ì—ˆìŠµë‹ˆë‹¤.\nì…í•™í—ˆê°€ë¥¼ ë°›ì€ ë‚ ë¶€í„° í¬í•­ì— ë„ì°©í•˜ì—¬ Slack Workspace ì•¡ì„¸ìŠ¤ë¥¼ ë°›ê¸° ì „ê¹Œì§€ì˜ stepë“¤ì„ ë‹¤ë£¹ë‹ˆë‹¤.\nSlack ì´ˆëŒ€ë¥¼ ë°›ì€ ì´í›„ì—ëŠ”, ìŠ¬ë™ ë‚´ì— ê¸°ì¬ ì˜ˆì •ì¸ ì˜¨ë³´ë”© ì ˆì°¨ë¥¼ ë”°ë¥´ì‹œë©´ ë©ë‹ˆë‹¤.\në¬¼ë¡ , ë³„ë„ë¡œ í•™ê³¼ì—ì„œ ìš”êµ¬í•˜ëŠ” ì ˆì°¨ë“¤ë„ ë”°ë¼ì£¼ì„¸ìš”.\nStill not sure? EffLì— ì˜¤ì‹¤ì§€, ë‹¤ë¥¸ ê³³ìœ¼ë¡œ ì§„í•™í•˜ì‹¤ì§€ ì•„ì§ ë¯¸ì •ì´ë¼ë©´ ì–´ë–¤ ê²ƒë„ ì§„í–‰í•˜ì§€ ë§ˆì‹œê¸° ë°”ëë‹ˆë‹¤. ê²°ì • ì´í›„ì— ì§„í–‰í•´ì£¼ì„¸ìš”.\ní˜„ ë²„ì „ì€ pre-alpha versionìœ¼ë¡œ, ì¼ë¶€ ì •ë³´ê°€ ëˆ„ë½ëì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤ ğŸ˜… Before Arrival # 1. GitHub Access PIì—ê²Œ ë©”ì¼ì„ ë³´ë‚´, ì‚¬ìš©í•˜ì‹œëŠ” GitHub ê³„ì •ì„ ì•Œë ¤ì£¼ì„¸ìš”. ì—°êµ¬ì‹¤ ì›¹í˜ì´ì§€ repositoryì— collaboratorë¡œ ì¶”ê°€ë“œë¦´ ì˜ˆì •ì…ë‹ˆë‹¤.\nGit? Git/GitHub ì´ìš©ì— ìµìˆ™í•˜ì‹œê¸¸ ê¸°ëŒ€í•©ë‹ˆë‹¤ ğŸ¥¹ ì˜ ëª¨ë¥´ì‹ ë‹¤ë©´ ì›¹ì— íŠœí† ë¦¬ì–¼ì´ ë§ìœ¼ë‹ˆ ê³µë¶€í•´ë‘ì„¸ìš” (e.g., ì—¬ê¸°).\n2. Present yourself in EffL Webpage ì—°êµ¬ì‹¤ í™ˆí˜ì´ì§€ ë‚´ ì„œë¸Œí˜ì´ì§€ë¡œ ë³¸ì¸ì˜ í˜ì´ì§€ë¥¼ ë§Œë“¤ì–´ì£¼ì„¸ìš”. ë§Œì•½ ë³¸ì¸ì˜ ì›¹í˜ì´ì§€ê°€ ì´ë¯¸ ìˆëŠ” ê²½ìš°, ë§í¬ë§Œ í•´ë‘ì…”ë„ ì¢‹ìŠµë‹ˆë‹¤.\ní˜„ì¬ ì—°êµ¬ì‹¤ í™ˆí˜ì´ì§€ëŠ” Hugoë¥¼ ì‚¬ìš©í•˜ê³  ìˆìœ¼ë©°, Markdown ë¬¸ì„œë¥¼ ì‘ì„±í•˜ëŠ” ê²ƒ ë§Œìœ¼ë¡œë„ ê°œì¸ ì†Œê°œ ì„œë¸Œí˜ì´ì§€ë¥¼ ì†ì‰½ê²Œ ë§Œë“¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì‚¬ìš©ë²•ì€ íŠœí† ë¦¬ì–¼ì„ ì°¸ê³ í•˜ì„¸ìš”.\nëŒ€ëµ, ë‹¤ìŒ ìˆœì„œë¥¼ ë”°ë¼ê°€ì‹œë©´ ë©ë‹ˆë‹¤.\nRepositoryì˜ content/docs/people/member/ ë””ë ‰í† ë¦¬ì— ìì‹ ì˜ ì´ë¦„ìœ¼ë¡œ ëœ ë¬¸ì„œë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤ (e.g., jaeho.md) ì´ˆë³´ìì˜ ê²½ìš°, ë‹¤ë¥¸ ë¶„ì´ ì´ë¯¸ ì‘ì„±í•˜ì‹  ë¬¸ì„œë¥¼ ë³µë¶™í•˜ì‹œë©´ ì¢‹ìŠµë‹ˆë‹¤. í—¤ë”ì—ì„œ ì´ë¦„ì„ ìˆ˜ì •í•˜ê³ , ë‚´ìš©ì„ ìì‹ ì„ ì˜ ì†Œê°œí•  ìˆ˜ ìˆëŠ” ë‚´ìš©ìœ¼ë¡œ ììœ ë¡­ê²Œ ìˆ˜ì •í•´ì£¼ì„¸ìš”. ì´ë¯¸ì§€ ë“±ì„ ì¶”ê°€í•˜ê³  ì‹¶ìœ¼ì‹  ê²½ìš° assets/ ì„œë¸Œë””ë ‰í† ë¦¬ë¥¼ ë§Œë“¤ì–´ ì €ì¥í•˜ì‹œë©´ ë  ê²ƒ ê°™ìŠµë‹ˆë‹¤. Repositoryì˜ content/docs/people/members.mdì— ìì‹ ì˜ ì´ë¦„ì„ ì¶”ê°€í•˜ê³ , ì›¹í˜ì´ì§€ë¥¼ ë§í¬í•´ì£¼ì„¸ìš”. ì»´í“¨í„°ì— Hugoê°€ ì„¤ì¹˜ë˜ì–´ ìˆëŠ” ê²½ìš°, ì •ìƒì ìœ¼ë¡œ ì›¹í˜ì´ì§€ê°€ ì‘ë™í•˜ëŠ” ê²ƒì„ í™•ì¸í•œ ë’¤ (ì˜ˆ: hugo server ì»¤ë§¨ë“œ ì´í›„ localhost ì ‘ì†) ì»´íŒŒì¼í•˜ì—¬ repositoryì— ì—…ë°ì´íŠ¸í•´ì£¼ì„¸ìš”. ì§ˆë¬¸. ê¶ê¸ˆí•œ ì‚¬í•­ì´ ìˆë‹¤ê±°ë‚˜, ì‹¤ìˆ˜ë¡œ ì›¹í˜ì´ì§€ë¥¼ í„°ëœ¨ë ¸ë‹¤ë©´ PIì—ê²Œ ë©”ì¼ë¡œ ì•Œë ¤ì£¼ì„¸ìš”.\n3. Purchase your Computers ë„ì°©í•˜ì‹œìë§ˆì ì¼ì„ ì‹œì‘í•˜ì‹¤ ìˆ˜ ìˆê²Œ ë¯¸ë¦¬ ì—°êµ¬ìš© ì»´í“¨í„°ë¥¼ êµ¬ì…í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤.\nì˜ˆì‚°. 2023ë…„ 10ì›” ê¸°ì¤€, ìµœëŒ€ 300ë§Œì›ì˜ ì˜ˆì‚°ì„ ë°°ì •í•  ì˜ˆì •ì…ë‹ˆë‹¤.\nìš©ë„. ë°ìŠ¤í¬íƒ‘/ë…¸íŠ¸ë¶/ëª¨ë‹ˆí„° êµ¬ì…ì— ì‚¬ìš©í•˜ì‹¤ ìˆ˜ ìˆê³ , í‚¤ë³´ë“œ/ë§ˆìš°ìŠ¤/ëª¨ë‹ˆí„°ì•” ë“±ì€ êµ¬ì…ì´ ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤.\nì†Œìœ ê¶Œ. í•™êµ ì†Œìœ ë¬¼ì´ê¸° ë•Œë¬¸ì— ì—°êµ¬ìš©ìœ¼ë¡œë§Œ ì‚¬ìš©í•´ì£¼ì‹œê³ , ì¡¸ì—… ì‹œ ë°˜ë‚©í•˜ì…”ì•¼ í•©ë‹ˆë‹¤. ê°œì¸ìš©ìœ¼ë¡œ ì‚¬ìš©í•˜ì‹¤ ë…¸íŠ¸ë¶ì€ ë”°ë¡œ êµ¬ë¹„í•˜ì‹œê¸¸ ê¶Œí•©ë‹ˆë‹¤.\nì¶”ì²œ. MacBook Pro í˜¹ì€ iMacì„ êµ¬ì…í•˜ì‹œëŠ” ê²ƒì„ ê°•ë ¥í•˜ê²Œ ì¶”ì²œë“œë¦½ë‹ˆë‹¤. ë”¥ëŸ¬ë‹ì„œë²„ì—ì„œ ì‚¬ìš©í•˜ëŠ” linux ë¬¸ë²•ì— ìµìˆ™í•´ì§€ê¸° ì¢‹ê³ , í„°ë¯¸ë„ ì‚¬ìš©ì´ í¸ë¦¬í•˜ë©°, ë³´ì•ˆê³¼ ì‹œìŠ¤í…œ ì•ˆì •ì„±ì´ ë›°ì–´ë‚˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. ê·¸ë¦¬ê³  PIê°€ ì•±ë“±ì´ì´ê¸° ë•Œë¬¸ì— co-workí•˜ê¸° í¸í•©ë‹ˆë‹¤ ğŸ˜…\nêµ¬ì…í•˜ê³  ì‹¶ìœ¼ì‹  ëª¨ë¸ì„ ê²°ì •í•˜ê³  ë‚˜ì„œ, ê²¬ì ì„œë¥¼ PIì—ê²Œ ì „ë‹¬í•´ì£¼ì„¸ìš”. ì—°êµ¬ë¹„ë¡œ êµ¬ì…í•˜ëŠ” ê²ƒì´ê¸° ë•Œë¬¸ì—, ì •í™•í•œ êµ¬ì…ì‹œê¸°ì™€ êµ¬ë§¤ì²˜ëŠ” ë³€ë™ì´ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤ (Appleì˜ ê²½ìš° êµìœ¡í• ì¸ì´ ì•„ì‰½ê²Œë„ ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤).\n4. Plan for Pohang Life í¬í•­ì˜ ê±°ì£¼ì˜µì…˜ë“¤ì„ ì•Œì•„ë³´ì‹œê³  í•©ë¥˜ì¼ì •ì„ PIì—ê²Œ ë©”ì¼ë¡œ ì•Œë ¤ì£¼ì„¸ìš”.\ní•™ê¸° ì‹œì‘ ì „ ë¯¸ë¦¬ í•©ë¥˜í•˜ì‹œëŠ” ê²½ìš°ì—ë„ ê¸°ìˆ™ì‚¬ë¥¼ ì´ìš©ê°€ëŠ¥í•œ ê²½ìš°ê°€ ìˆìŠµë‹ˆë‹¤. í•™ê³¼ì‚¬ë¬´ì‹¤ì„ í†µí•´ ì´ìš©ê°€ëŠ¥í•œ ì˜µì…˜ì„ ë¬¸ì˜í•´ì£¼ì„¸ìš”.\nì¶”ì²œ. ì¬íƒê·¼ë¬´ ê²½í—˜ì´ ë§ì§€ ì•Šìœ¼ì‹œë‹¤ë©´, ê¸°ìˆ™ì‚¬ ì´ìš©ì„ ì¶”ì²œë“œë¦½ë‹ˆë‹¤. ì´ì™¸ì—ë„ ìœ ê°•, SKë·°, íš¨ìì‹œì¥ ë“±ì— ê´œì°®ì€ housing optionì´ ìˆë‹¤ê³  ì•Œê³  ìˆìŠµë‹ˆë‹¤. ì—°êµ¬ì‹¤ì€ ì œ2ê³µí•™ê´€ì— ìœ„ì¹˜í•´ ìˆìœ¼ë‹ˆ ìœ„ì¹˜ ì„ ì •ì— ì°¸ê³ í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤. ë„¤ì´ë²„ ì§€ë„\n5. Keep in Touch PIì—ê²Œ ì£¼ê¸°ì ìœ¼ë¡œ ì—°ë½ì„ ì·¨í•´ ì—°êµ¬ ë° ê´€ë ¨ ê³µë¶€ë¥¼ ë¯¸ë¦¬ ì§„í–‰í•˜ì‹œë©´ ëŒ€í•™ì› ìƒí™œì—ì„œ í›¨ì”¬ ë” ë§ì€ ê²ƒì„ ì–»ì–´ê°€ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\në§ì€ ê²½ìš°, PIê°€ ëª‡ ê°€ì§€ ë¯¸ì…˜ì„ ë“œë¦´ ìˆ˜ ìˆì„ ê²ƒì…ë‹ˆë‹¤\u0026hellip; ğŸ¤”\nAfter Arrival # 6. Let Jaeho Know PIì—ê²Œ ë©”ì¼ì„ ë³´ë‚´, ë¬´ì‚¬íˆ ë„ì°©í–ˆë‹¤ëŠ” ì‚¬ì‹¤ê³¼ ì–¸ì œ ì—°êµ¬ì‹¤ì— ì²« ë°©ë¬¸ì„ í•˜ì‹¤ ì§€ ì•Œë ¤ì£¼ì„¸ìš”.\në³´í†µì€ PIì™€ ê°„ë‹¨í•˜ê²Œ ë¯¸íŒ…ì„ í•œ ë’¤, PIê°€ ì—°êµ¬ì‹¤ë¡œ ëª¨ì‹œê³  ê°€ì„œ ê°„ë‹¨í•˜ê²Œ ì†Œê°œë¥¼ ë“œë¦´ ì˜ˆì •ì…ë‹ˆë‹¤.\nìœ„ì¹˜. PIì˜ ì˜¤í”¼ìŠ¤ëŠ” ì œ2ê³µí•™ê´€ 407í˜¸ì…ë‹ˆë‹¤.\n7. Desk Assignment ì—°êµ¬ì‹¤ì— ì˜¤ì…”ì„œ ìë¦¬ë¥¼ ê³ ë¥´ì„¸ìš”. ë¹ˆ ìë¦¬ ì¤‘ì—ì„œ ë§ˆìŒì— ë“œëŠ” ê³³ì„ ì„ íƒí•˜ì‹œë©´ ë©ë‹ˆë‹¤.\n(TODO: ëˆ„ê°€ ì–´ë”” ì•‰ëŠ”ì§€ ê¸°ë¡í•˜ëŠ” ì‹œìŠ¤í…œ)\nìœ„ì¹˜. EffL Labì€ ì œ2ê³µí•™ê´€ 404í˜¸ì— ìˆìŠµë‹ˆë‹¤.\nì¬ë°°ì •. ë§¤ë…„ ê°€ì„í•™ê¸° ì´ˆ, ì²«ë²ˆì§¸ Group Meetingì—ì„œ ì¬ë°°ì •ì„ ì§„í–‰í•  ì˜ˆì •ì…ë‹ˆë‹¤.\n8. Request Slack Access ì—°êµ¬ì‹¤ Slackì´ ìˆìŠµë‹ˆë‹¤. PIì—ê²Œ ë©”ì¼ì„ ë³´ë‚´, ì–´ë–¤ ì´ë©”ì¼ ê³„ì •ìœ¼ë¡œ ì´ˆëŒ€ë©”ì¼ì„ ë³´ë‚´ë©´ ë˜ëŠ”ì§€ ì•Œë ¤ì£¼ì„¸ìš”.\n(TODO: Slack í•™ìƒ ê´€ë¦¬ì)\nì£¼ì˜. ì—°êµ¬ì‹¤ ë‚´ ì†Œí†µì€ Slackì„ ì´ìš©í•´ì£¼ì‹œê³ , ë¶ˆê°€í”¼í•œ ê²½ìš°ë¥¼ ì œì™¸í•˜ê³  ì „í™”/ë¬¸ì/ì¹´í†¡ ë“±ì€ ì‚¼ê°€ì£¼ì„¸ìš”.\n9. After Getting Slack Access ì´í›„ì˜ stepë“¤ì€ Slack ë‚´ ì˜¨ë³´ë”© ê¸°ëŠ¥ì„ í™œìš©í•˜ì—¬ ì•ˆë‚´ë¥¼ í•  ì˜ˆì •ì…ë‹ˆë‹¤.\nì´ ê¸°ëŠ¥ì€ ì•„ì§ê¹Œì§€ëŠ” êµ¬í˜„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤ ã… ã… \nMeanwhile, Office Team Lead (ì´ì§€ìš´ë‹˜)ì„ í†µí•´ í•„ìš”í•œ ê²ƒë“¤ì„ ì—¬ì­¤ë³´ì„¸ìš”.\n(TODO: í”„ë¦°í„°, ì™€ì´íŒŒì´, ì—°êµ¬ì‹¤ ë¹„ë°€ë²ˆí˜¸, ì„œë²„ ë° ì„œë²„ì‹¤ ì•¡ì„¸ìŠ¤, í¬ë¹„ìŠ¤, MS Teams)\n"},{"id":10,"href":"/olds/","title":"Past news","section":"EffL Lab","content":" Past news # 2024 # (Sep) Three papers accepted at EMNLP 2024 ğŸ‡ºğŸ‡¸ (Sep) Received Gemma 2 Academic Program Credit Award from Google ($10,000) ğŸ’° (Jul) An undergrad intern (Inkwan) will be a Korean Presidential Science Fellow ğŸ’° (Jul) One paper accepted at ECCV 2024 ğŸ‡®ğŸ‡¹ (Jun) Seungwoo and Hagyeong defended their M.S. theses ğŸ›¡ï¸ (May) Two papers accepted at ICML 2024 ğŸ‡¦ğŸ‡¹ (Mar) One paper accepted at NAACL 2024 ğŸ‡²ğŸ‡½ (Mar) One paper accepted at IEEE S\u0026amp;P 2024 (Feb) Two papers accepted at CVPR 2024 ğŸ‡ºğŸ‡¸ (1 oral, 1 highlight) (Feb) Junwon and Jiwoon earned their M.S. degrees ğŸ“ 2023 # (Oct) One paper accepted at NeurIPS 2023 ğŸ‡ºğŸ‡¸ (Sep) Jaeho and Seungwoo started working at Google. (Jun) One paper accepted at ICML 2023 ğŸŒº (Jun) One paper accepted at TMLR "},{"id":11,"href":"/docs/How-to-Join/intern_program/","title":"Internship Program","section":"How to Join?","content":" Internship Program # We treat interns differently with respect to your skill levels.\n(1) Level System # We\u0026rsquo;ll put you into one of three groups: L1, L2, L3, depending on your level of expertise.\nL1. You know very little about ML/DL. Goal: Complete an online DL coursework (e.g., this, this or this). L2. You are familiar with DL, but not research. Goal: Select a research topic and learn how to read papers. L3. You can read papers, and code in PyTorch/Jax. Goal: Read papers up to the research frontier, and start doing research. (2) Weekly Meeting # You will have a weekly meeting with Jaeho and other interns.\nIn every other meeting (i.e., twice a month), you are expected to do the following:\nL1. Cover a deep learning video lecture; provide your own version of summary. L2. Summarize a research paper, that is aligned with your interest. L3. Present your research progress. (3) Promotion # If you are L1 or L2, and feel you\u0026rsquo;re ready to get promoted, apply for a defense.\nIn the very next meeting, we will test if you have what it takes to become the next level.\n(4) Small Perks # Your own desk, located in the graduate student office. Free use of capsule coffee machines, printers, and Wi-Fi. Non-mandatory invitations to group lunches and dinners. "},{"id":12,"href":"/docs/people/member/hagyeong/","title":"Hagyeong Lee","section":"People","content":" Hagyeong Lee # (say something)\n"},{"id":13,"href":"/docs/people/member/hyunjong/","title":"Hyunjong Ok","section":"People","content":" Hyunjong Ok # ğŸ” Research Interests # Efficient LLM, Multi-modal learning, Audio Technologies\nğŸ« Education # Pohang University of Science and Technology (POSTECH)\n(2024.02 ~ Present) M.S. candidate in Graduate School of AI KyungHee University (KHU)\n(2019.03 ~ 2023.08) B.S in Software Convergence ğŸ¢ Experience # (2023.03 ~ 2023.09) AI Intern, Naver CLOUD (2022.03 ~ 2023.02) Undergraduate Researcher, NLP Lab, KyungHee University (2022.07 ~ 2022.08) AI project, Hyundai Motor (2021.11 ~ 2022.02) Data Engineer Intern , Ingkle ğŸ“œ Publication # Hyunjong Ok, Taeho Kil, Sukmin Seo, Jaeho Lee, \u0026ldquo;SCANNER: Knowledge-Enhanced Approach for Robust Multi-modal Named Entity Recognition of Unseen Entities\u0026rdquo;, NAACL, 2024\nHyunjong Ok, \u0026ldquo;FinTree: Financial Dataset Pretrain Transformer Encoder forRelation Extraction\u0026rdquo;, SIGIR-Workshop (KDF), 2023\nHyunjong Ok, Seong-Bae Park, \u0026ldquo;POST-TRAINED LANGUAGE MODEL ADAPTIVE TO EXTRACTIVE SUMMARIZATION OF LONG SPOKEN DOCUMENTS\u0026rdquo;, ICASSP (Grand Challenge), 2023\nğŸ† Award # (2022.10) í•œêµ­ì–´ AI ê²½ì§„ëŒ€íšŒ: ASR TRACK (1ST PLACE: KOREA MINISTER\u0026rsquo;S AWARD), Ministry of Science and ICT (2022.12) KLAID LEGAL JUDGMENT PREDICTION CHALLENGE (1ST PLACE), Law \u0026amp; Company (2023.02) ICASSP 2023 MUG GRAND CHALLENGE 2ND TRACK (1ST PLACE), Alibaba DAMO Academy (2022.12) ìœ ì „ì²´AI ê²½ì§„ëŒ€íšŒ (2ND PLACE), Infoboss (2023.06) SIGIR 2023 KDF WORKSHOP SHARED TASK (2ND PLACE), J.P. Morgan AI Research (2022.12) ë¬¸ì¥ ìœ í˜• ë¶„ë¥˜ AI ê²½ì§„ëŒ€íšŒ (4TH PLACE), Sungkyunkwan University (2023.11) í•œêµ­ì–´ AI ê²½ì§„ëŒ€íšŒ: ASR TRACK (4TH PLACE), Ministry of Science and ICT (2023.12) êµ­ë°© AI ê²½ì§„ëŒ€íšŒ (4TH PLACE), Ministry of National Defense (2023.10) COMMONLIT - EVALUATE STUDENT SUMMARIES (SILVER MEDAL), Kaggle (2022.03) FEEDBACK PRIZE - EVALUATING STUDENT WRITING (SILVER MEDAL), Kaggle (2022.07) KORE 2022 (BRONZE MEDAL), Kaggle ğŸ“ Contact # mail : r7play@postech.ac.kr github : https://github.com/HJ-Ok "},{"id":14,"href":"/docs/people/member/jiwoon/","title":"Jiwoon Lee","section":"People","content":" Jiwoon Lee # ğŸ” Research Interests # Model merging, Federated learning, Knowledge distillation\nğŸ« Education # Pohang University of Science and Technology (POSTECH) (2015.03 ~ 2022.02) B.S. in Electrical Engineering -(2018.10 ~ 2020.05) Military service (2022.02 ~ Present) M.S. candidate in Electrical Engineering ğŸ¢ Experience # (2017.06 ~ 2017.07) Research Intern, Preceding Patterning Team, SK Hynix (2018.06 ~ 2018.08) Research Intern, Embedded System Architecture Lab, POSTECH ğŸ“š Publication # Jiwoon Lee, Jaeho Lee, \u0026ldquo;Debiased Distillation by Transplanting the Last Layer\u0026rdquo;, ArXiv preprint, 2023. arxiv - Preliminary version presented in Workshop on Image Processing and Image Understanding 2023. ğŸ“ Contact # mail : jwlee9702@postech.ac.kr, easy9702@gmail.com Social accounts : linkedin\n"},{"id":15,"href":"/docs/people/member/jiyunbae/","title":"Jiyun Bae","section":"People","content":" Jiyun Bae # Hi there, I\u0026rsquo;m Jiyun ğŸ‘‹\nğŸ” Research Interests # Text/Visual Prompts, Bias, Fairness, LLMs\nğŸ« Education # Seoul Women University (SWU)\n(2020.03 ~ 2024.02) B.S in Information Security Pohang University of Science and Technology (POSTECH)\n(2024.02 ~ Present) M.S. candidate in Graduate School of AI ğŸ¢ Experience # (2022.01 ~ 2022.03) Software Developer Intern, Ericsson-LG (2022.07 ~ 2022.09) NLP Researcher Intern, S2W (2023.06. ~ 2024.02) Undergraduate Research Fellowship, Efficient Learning Lab, POSTECH âœ¨ Patents # (2021.12) OBJECT RECOGNITION MIRROR AND OBJECT RECOGNITION METHOD, 10-2021-0190705 â›³ï¸ Activity # (2022.08 ~ 2023.07) Google Developer Student Clubs SWU 1st LEAD, Awarded Certificate of Excellence (2021.11) Girls in ICT 2021 Hackathon(Ericsson-LG), Best Awards ğŸ€ Volunteer Experience # (2022.09 ~ Present) Ambassador of Google\u0026rsquo;s Women Techmakers ğŸ“ Contact # mail : jiyun.bae@postech.ac.kr github : https://github.com/jiyunBae007 Social accounts : linkedin\n"},{"id":16,"href":"/docs/people/member/junwon/","title":"Junwon Seo","section":"People","content":" Junwon Seo # ğŸ” Research Interests # Neural Field, Training Efficiency, Implicit Bias of SGD\nğŸ« Education # (2018.03 ~ 2022.02) B.S. in Computer Science and Engineering, Chung-Ang University\n(2022.02 ~ Present) M.S. candidate in Electrical Engineering, POSTECH\nğŸ¢ Experience # (2020.06 ~ 2021.09) Research Intern, Data Intelligence Lab, Chung-Ang University ğŸ“ Contact # mail : junwon.seo@postech.ac.kr, junwon.seo97@gmail.com\nSocial accounts : linkedin\n"},{"id":17,"href":"/docs/people/member/juyun/","title":"Juyun Wee","section":"People","content":" Juyun Wee # ğŸ” Research Interests # Test-Time Training, Time-Series Forecasting\nğŸ« Education # Pohang University of Science and Technology (POSTECH) (2018.03 ~ 2023.08) B.S. in Electrical Engineering (2023.09 ~ Present) M.S. candidate in Electrical Engineering University of Stuttgart, Germany (2022.08 ~ 2023.01) Exchange Student in Electrical Enginerring ğŸ¢ Experience # (2023.03 ~ 2017.07) Research Intern, Efficient Learning Laboratory, POSTECH (2021.09 ~ 2017.01) Research Intern, Advanced Information Systems Laboratory, POSTECH (2020.06 ~ 2020.08) Startup Intern, R\u0026amp;D Team, Reziena (2019.06 ~ 2019.08) Startup Intern, R\u0026amp;D Team, Dodo tDo ğŸ“ Contact # mail : jywee@postech.ac.kr\n"},{"id":18,"href":"/docs/people/member/minjae/","title":"Minjae Park","section":"People","content":" Minjae Park # ğŸ” Research Interests # State-Space Model, Model compression(Pruning, Quantization)\nğŸ« Education # Sogang University\n(2020.03 ~ 2024.02) B.S in Electrical \u0026amp; Electronics Engineering Pohang University of Science and Technology (POSTECH)\n(2024.03 ~ Present) M.S. \u0026amp; Ph.D candidate in Electrical \u0026amp; Electronics Engineering ğŸ¢ Experience # (2023.06 ~ 2023.12) Research Intern, Man Machine Interface Lab, Sogang University ğŸ“ Contact # mail : minjae0047@postech.ac.kr, mjae.park@gmail.com github : https://github.com/tada0347/ "},{"id":19,"href":"/docs/people/member/minkyu/","title":"Minkyu Kim","section":"People","content":" Minkyu Kim # Hello! ğŸ‘‹\nğŸ” Research Interests # Prefix Tuning, Image Compression, Multi-modal learning\nğŸ« Education # Chung-Ang University (CAU)\n(2017.03 ~ 2023.02) B.S in Electrical \u0026amp; Electronics Engineering Pohang University of Science and Technology (POSTECH)\n(2023.02 ~ Present) M.S. candidate in Graduate School of AI ğŸ¢ Experience # (2021.09 ~ 2021.12) Research Intern, Computer Vision Lab, Korea University (2022.07 ~ 2022.08) Undergraduate Research Fellowship, Algorithmic Machine Intelligence Lab, POSTECH ğŸ“œ Publication # Hagyeong Lee*, Minkyu Kim*, Jun-Hyuk Kim, Seungeon Kim, Dokwan Oh, Jaeho Lee, \u0026ldquo;Neural Image Compression with Text-guided Encoding for both Pixel-level and Perceptual Fidelity\u0026rdquo;, ICML, 2024\nMinkyu Kim*, Kim Sung-Bin*, Tae-Hyun Oh, \u0026ldquo;PREFIX TUNING FOR AUTOMATED AUDIO CAPTIONING\u0026rdquo;, ICASSP, 2023 [project page]\nâ¡ï¸ Selected as Oral presentation \u0026amp; Media coverage: covered by Yonhap News, SBS, and many local media\nğŸ“ Services # Peer review, IEEE/ACM Transactions on Audio, Speech, and Language Processing, 2023\nğŸ“ Contact # mail : minkyu.kim@postech.ac.kr, minkyu4506@gmail.com github : https://github.com/MinkyuKim26 Social accounts : linkedin\n"},{"id":20,"href":"/docs/people/member/sangyoon/","title":"Sangyoon Lee","section":"People","content":" Sangyoon Lee # ğŸ” Research Interests # Fast Neural Fields, Editable Neural Fields, Neural Compression\nğŸ« Education # Pohang University of Science and Technology (POSTECH) (2018.02 - 2023.08) B.S. in Computer Science and Engineering (2024.03 - Present) M.S. candidate in Graduate School of AI ğŸ¢ Experience # (2023.07 - 2024.02) Research Intern in Efficient Learning Lab, POSTECH (2022.07 - 2023.06) Research Intern in Computer Vision Lab, POSTECH (2022.03 - 2022.06) Research Intern in Data System Lab, POSTECH ğŸ“œ Publication # In Search of a Data Transformation that Accelerates Neural Field Training\nJunwon Seo*, Sangyoon Lee*, Kwang In Kim, and Jaeho Lee\nCVPR 2024 Oral (top 0.78%) (NeurIPS 2023 Workshop: Attributing Model Behavior at Scale)\nğŸ“ Contact # mail : sangyoon.lee@postech.ac.kr\n"},{"id":21,"href":"/docs/people/member/seungwoo/","title":"Seungwoo Son","section":"People","content":" Seungwoo Son # test\n"},{"id":22,"href":"/docs/people/member/yuji/","title":"Yuji Byun","section":"People","content":" Yuji Byun # ğŸ” Research Interests # Image Inpainting, Computer Vision\nğŸ« Education # (2011.02 ~ 2015.02) B.S. in Computer Science at Naval Academy (2023.02 ~ Present) M.S. candidate in Defense Science at POSTECH ğŸª– Experience # (2015. 03 ~ Present) ROKMC communication officer ğŸ“ Contact # mail : yujibyun@postech.ac.kr\n"}]