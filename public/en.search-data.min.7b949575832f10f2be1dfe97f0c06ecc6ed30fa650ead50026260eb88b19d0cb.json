[{"id":0,"href":"/docs/research/focus/","title":"Focus","section":"Research","content":"The long-term goal of EffL is to make AI more responsible\u0026ndash;accessible, sustainable, and righteous.\nAs the first step, we are focusing on various facets of Efficient ML, which could help us make AI equally accessible to anybody on Earth, with no extreme carbon emission.\nIn particular, we work on three dimensions:\nInference. We develop fast, low-resource methods to serve massive multimodal AI. Area: Model Compression, Advanced Decoding, Batch Scheduling Training. We resolve the compute and memory bottlenecks for training large-scale models. Area: Meta-Learning, Model Merging, Parameter-Efficient Fine-Tuning Data Dimension. We design principled algorithms to handle data with extremely high dimensionality (e.g., video, astronomy). Area: Data Compression, High-res Video Processing, Time-Series Forecasting For more, see our papers.\n"},{"id":1,"href":"/docs/research/papers/","title":"Papers","section":"Research","content":"Under construction.\n"},{"id":2,"href":"/docs/people/","title":"People","section":"Docs","content":" Jaeho Lee # Assistant Professor @ POSTECH EE (22.03\u0026ndash;)\nPrincipal Investigator @ EffL (22.03\u0026ndash;)\nVisiting Researcher @ Google (23.09\u0026ndash;)\nwebpage, mail, twitter\nSeung-Ah Song # Administrative Staff @ EffL (22.03\u0026ndash;)\nmail\nJiwoon Lee # Efficient ML in the Wild üêä\nGraduate Student @ POSTECH EE (22.03\u0026ndash;)\nOffice Team Lead @ EffL\nKeywords: Model Merging, Federated Learning, Knowledge Distillation\nmail\nJunwon Seo # Blazing-Fast Neural Field Generation üî•\nGraduate Student @ POSTECH EE (22.03\u0026ndash;)\nInfrastructure Team Lead @ EffL\nKeywords: Neural Field, Training Efficiency, Implicit Bias of SGD\nmail\nSeungwoo Son # Compress Gigantic Transformers, but Efficiently ü§ë\nGraduate Student @ POSTECH EE (22.03\u0026ndash;)\nStudent Intern @ Google\nKeywords: Large Language Models, Masked Modeling, Knowledge Distillation\nmail, twitter\nHagyeong Lee # Data Compression, but for more than what we see üîÆ\nGraduate Student @ POSTECH EE (22.09\u0026ndash;)\nAcademic Team Lead @ EffL\nKeywords: Data Compression, Model Bias, Visual-Language Model\nwebpage, mail, twitter\nMinkyu Kim # Harnessing Language Models for Multimodal Tasks üéôÔ∏è\nGraduate Student @ POSTECH AI (23.03\u0026ndash;)\nKeywords: Prefix Tuning, Multimodal Learning, Data Compression\nmail, blog, recent project\nYu Ji Byun # Completing High-Resolution Videos with Low Resources üìπ\nGraduate Student @ POSTECH Defense Science (23.03\u0026ndash;)\nCaptain @ ROK Marine Corps\nKeywords: High-res Videos, Image Inpainting, Computer Vision\nmail\nJuyun Wee # Adaptive Processing of Extremely Long Data Sequences ‚è≥\nGraduate Student @ POSTECH EE (23.09\u0026ndash;)\nKeywords: Time-Series Forecasting, Test-Time Training, Self-supervised Learning\nmail\n"}]