---
title: Papers
type: docs
bookToc: true
weight: 2
---


We mainly target top-tier ML conferences, e.g., NeurIPS / ICML / ICLR.  
Sometimes, we also submit to domain-specific venues that involve ML, e.g., Vision/Language/Speech.  

---

### **2024**

[**The Role of Masking for Efficient Supervised Knowledge Distillation of Vision Transformers**](https://arxiv.org/abs/2302.10494)  
Seungwoo Son, Jegwang Ryu, Namhoon Lee, and Jaeho Lee  
**ECCV 2024** (ICLR 2023 workshop: Sparsity in Neural Networks, IPIU 2023 `Oral` `ü•â`)  

<div style="float: left; margin-right: 20px; margin-bottom: 10px; margin-top: 10px;">
    {{< figure src="/images/papers/2024/TACO.png" alt="." width="100" height="100" >}}  
</div>

[**Neural Image Compression with Text-guided Encoding for both Pixel-level and Perceptual Fidelity**](https://arxiv.org/abs/2403.02944)  
Hagyeong Lee<sup>1st</sup>, Minkyu Kim<sup>1st</sup>, Jun-Hyuk Kim, Seungeon Kim, Dokwan Oh, and Jaeho Lee  
**ICML 2024**  


<div style="float: left; margin-right: 20px; margin-bottom: 10px; margin-top: 10px;">
    {{< figure src="/images/papers/2024/hybrid_sh.png" alt="." width="100" height="100" >}}  
</div>

[**Hybrid Neural Representations for Spherical Data**](https://arxiv.org/abs/2402.05965)  
Hyomin Kim, Yunhui Jang, Jaeho Lee, and Sungsoo Ahn  
**ICML 2024**  
#####
---
<div style="float: left; margin-right: 20px; margin-bottom: 10px; margin-top: 10px;">
    {{< figure src="/images/papers/2024/dt4neural-field.png" alt="." width="100" height="100" >}}  
</div>

[**In Search of a Data Transformation that Accelerates Neural Field Training**](https://arxiv.org/abs/2311.17094)  
Junwon Seo<sup>1st</sup>, Sangyoon Lee<sup>1st</sup>, Kwang In Kim, and Jaeho Lee  
**CVPR 2024** `Oral (top 0.78%)` (NeurIPS 2023 Workshop: Attributing Model Behavior at Scale)  
[[code](https://github.com/effl-lab/DT4Neural-Field)] [[demo](https://huggingface.co/spaces/lyunm1206/Interactive_Loss_Landscapes)]
#####
---
<div style="float: left; margin-right: 20px; margin-bottom: 10px; margin-top: 10px;">
    {{< figure src="/images/papers/2024/visual_biases.png" alt="." width="100" height="100" >}}  
</div>

[**Discovering and Mitigating Visual Biases through Keyword Explanation**](https://arxiv.org/abs/2301.11104)  
Younghyun Kim, Sangwoo Mo, Minkyu Kim, Kyungmin Lee, Jaeho Lee, and Jinwoo Shin  
**CVPR 2024** `Highlight (top 2.8%)` (ICML 2023 Workshop: Spurious Correlations, Invariance, and Stability)  
[[code](https://github.com/alinlab/b2t)]  
#####
---
<div style="float: left; margin-right: 20px; margin-bottom: 15px; margin-top: 10px;">
    {{< figure src="/images/papers/2024/scanner.png" alt="." width="100" height="100" >}}  
</div>

[**SCANNER: Knowledge-Enhanced Approach for Robust Multi-modal Named Entity Recognition of Unseen Entities**](https://arxiv.org/abs/2404.01914)  
Hyunjong Ok, Taeho Kil, Sukmin Seo, and Jaeho Lee  
**NAACL 2024**
#####
---
<div style="float: left; margin-right: 20px; margin-bottom: 10px; margin-top: 10px;">
    {{< figure src="/images/papers/2024/fewshot_unlearning.png" alt="." width="100" height="100" >}}  
</div>

[**Few-shot Unlearning**](https://arxiv.org/abs/2205.15567)  
Youngsik Yoon, Jinhwan Nam, Hyojeong Yun, Jaeho Lee, Dongwoo Kim, and Jungseul Ok  
**IEEE S&P 2024**
#####
---
<div style="float: left; margin-right: 20px; margin-bottom: 10px; margin-top: 10px;">
    {{< figure src="/images/papers/2024/semantic_communication.png" alt="." width="100" height="100" >}}  
</div>

[**Attention-aware Semantic Communications for Collaborative Inference**](https://arxiv.org/abs/2404.07217)  
Jiwoong Im, Nayoung Kwon, Taewoo Park, Jiheon Woo, Jaeho Lee, and Yongjune Kim  
Accepted for publication at **IEEE IoT Journal** (also at IEEE Communication Theory Workshop 2024)  
#####

[**Prefixing Attention Sinks can Mitigate Activation Outliers for Large Language Model Quantization**](https://arxiv.org/abs/2406.12016)  
Seungwoo Son, Wonpyo Park, Woohyun Han, Kyuyeun Kim, and Jaeho Lee  
arXiv preprint 2406.12016  

[**Decoding with Limited Teacher Supervision Requires Understanding When to Trust the Teacher**](https://arxiv.org/abs/2406.18002)  
Hyunjong Ok, Jegwang Ryu and Jaeho Lee  
arXiv preprint 2406.18002  

[**Towards Federated Low-Rank Adaptation with Rank-Heterogeneous Communication**](https://arxiv.org/abs/2406.17477)  
Yuji Byun and Jaeho Lee  
arXiv preprint 2406.17477  

[**Rethinking Pruning Large Language Models: Benefits and Pitfalls of Reconstruction Error Minimization**](https://arxiv.org/abs/2406.15524)  
Sungbin Shin, Wonpyo Park, Jaeho Lee, and Namhoon Lee  
arXiv preprint 2406.15524  

### **2023**
[**Learning Large-scale Neural Fields via Context Pruned Meta-learning**](https://arxiv.org/abs/2302.00617)  
Jihoon Tack, Subin Kim, Sihyun Yu, Jaeho Lee, Jinwoo Shin, and Jonathan R. Schwarz  
**NeurIPS 2023** (ICLR 2023 Workshop: Neural Fields across Fields)

[**Modality-Agnostic Variational Compression of Implicit Neural Representations**](https://openreview.net/forum?id=bBXCCSoVQZ)  
Jonathan R. Schwarz, Jihoon Tack, Yee Whye Teh, Jaeho Lee, and Jinwoo Shin  
**ICML 2023** (ICLR 2023 Workshop: Neural Fields across Fields)

[**Breaking the Spurious Causality of Conditional Generation via Fairness Intervention with Corrective Sampling**](https://openreview.net/forum?id=VV4zJwLwI7)  
Junhyun Nam, Sangwoo Mo, Jaeho Lee, and Jinwoo Shin  
**TMLR 2023** (ICML 2023 Workshop: Spurious Correlations, Invariance, and Stability)

[**Semi-Ensemble: A Simple Approach to Over-Parameterize Model Interpolation**](https://unireps.org)  
Jiwoon Lee and Jaeho Lee  
**NeurIPS 2023 Workshop**: Unifying Representations in Neural Models

[**On the Effectiveness of Sharpness-aware Minimization with Large Mini-batches**](https://icml.cc/virtual/2023/25899)  
Jinseok Chung, Seonghwan Park, Jaeho Lee, and Namhoon Lee  
**ICML 2023 Workshop**: High-Dimensional Learning Dynamics

[**Communication-Efficient Split Learning via Adaptive Feature-wise Compression**](https://arxiv.org/abs/2307.10805)  
Yongjeong Oh, Jaeho Lee, Christopher G. Brinton, and Yo-Seb Jeon  
arXiv 2307.10805

[**Debiased Distillation by Transplanting the Last Layer**](https://arxiv.org/abs/2302.11187)  
Jiwoon Lee and Jaeho Lee  
arXiv 2302.11187 (IPIU 2023)


### **2022**

[**Scalable Neural Video Representations with Learnable Positional Features**](https://openreview.net/forum?id=OxfI-3i5M8g)  
Subin Kim, Sihyun Yu, Jaeho Lee, and Jinwoo Shin  
**NeurIPS 2022**   
[[project page](https://subin-kim-cv.github.io/NVP/)]

[**Meta-learning with Self-improving Momentum Targets**](https://openreview.net/forum?id=FCNMbF_TsKm)  
Jihoon Tack, Jongjin Park, Hankook Lee, Jaeho Lee and Jinwoo Shin  
**NeurIPS 2022**

[**Spread Spurious Attribute: Improving Worst-Group Accuracy with Spurious Attribute Estimation**](https://openreview.net/forum?id=_F9xpOrqyX9)  
Junhyun Nam, Jaehyung Kim, Jaeho Lee, and Jinwoo Shin  
**ICLR 2022**

[**Zero-shot Blind Image Denoising via Implicit Neural Representations**](https://arxiv.org/abs/2204.02405)  
Chaewon Kim, Jaeho Lee, and Jinwoo Shin  
arXiv 2204.02405


### **2021**

[**Meta-learning Sparse Implicit Neural Representations**](https://openreview.net/forum?id=Tn0PnRY877g)  
Jaeho Lee, Jihoon Tack, Namhoon Lee, and Jinwoo Shin  
**NeurIPS 2021**

[**Co2L: Contrastive Continual Learning**](https://openaccess.thecvf.com/content/ICCV2021/html/Cha_Co2L_Contrastive_Continual_Learning_ICCV_2021_paper.html)  
Hyuntak Cha, Jaeho Lee, and Jinwoo Shin  
**ICCV 2021**

[**Provable Memorization via Deep Neural Networks using Sub-linear Parameters**](https://proceedings.mlr.press/v134/park21a.html)  
Sejun Park, Jaeho Lee, Chulhee Yun, and Jinwoo Shin  
**COLT 2021** (DeepMath 2020 `Oral`)

[**Minimum Width for Universal Approximation**](https://openreview.net/forum?id=O-XJwyoIF-k)  
Sejun Park, Chulhee Yun, Jaeho Lee, and Jinwoo Shin  
**ICLR 2021** `Spotlight` (DeepMath 2020 `Oral`)

[**Layer-adaptive Sparsity for the Magnitude-based Pruning**](https://openreview.net/forum?id=H6ATjJ0TKdf)  
Jaeho Lee, Sejun Park, Sangwoo Mo, Sungsoo Ahn, and Jinwoo Shin  
**ICLR 2021**

[**MASKER: Masked Keyword Regularization for Reliable Text Generation**](https://ojs.aaai.org/index.php/AAAI/article/view/17601)  
Seung Jun Moon, Sangwoo Mo, Kimin Lee, Jaeho Lee, and Jinwoo Shin  
**AAAI 2021**

[**Greedyprune: Layer-wise Optimization Algorithms for Magnitude-based Pruning**](https://sites.google.com/view/sparsity-workshop-2021/accepted-papers)  
Vinoth Nandakumar and Jaeho Lee  
**Sparse Neural Network Workshop 2021**

### **2020**

[**Learning Bounds for Risk-sensitive Learning**](https://proceedings.neurips.cc/paper/2020/hash/9f60ab2b55468f104055b16df8f69e81-Abstract.html)  
Jaeho Lee, Sejun Park, and Jinwoo Shin  
**NeurIPS 2020**

[**Learning from Failure: Training Debiased Classifier from Biased Classifier**](https://papers.nips.cc/paper_files/paper/2020/hash/eddc3427c5d77843c2253f1e799fe933-Abstract.html)  
Junhyun Nam, Hyuntak Cha, Sungsoo Ahn, Jaeho Lee, and Jinwoo Shin  
**NeurIPS 2020**

[**Lookahead: A Far-sighted Alternative of Magnitude-based Pruning**](https://openreview.net/forum?id=ryl3ygHYDB)  
Sejun Park, Jaeho Lee, Sangwoo Mo, and Jinwoo Shin  
**ICLR 2020**

### **Pre-2020**

[**Learning Finite-dimensional Coding Schemes with Nonlinear Reconstruction Maps**](https://epubs.siam.org/doi/10.1137/18M1234461)  
Jaeho Lee and Maxim Raginsky  
**SIMODS 2019**

[**Minimax Statistical Learning with Wasserstein Distances**](https://papers.nips.cc/paper_files/paper/2018/hash/ea8fcd92d59581717e06eb187f10666d-Abstract.html)  
Jaeho Lee and Maxim Raginsky  
**NeurIPS 2018** `Spotlight`

[**On MMSE Estimation from Quantized Observations in the Nonasymptotic Regime**](https://ieeexplore.ieee.org/document/7282992)  
Jaeho Lee, Maxim Raginsky, and Pierre Moulin  
**ISIT 2015**


### **Domestic Posters üêØ**

---

#### **An Empirical Study on the Bias of Generative Image Compression**  
Hagyeong Lee and Jaeho Lee  
**IPIU 2023**

#### **Is Sparse Identification Model Sufficiently Biased?**  
Junwon Seo and Jaeho Lee  
**IPIU 2023**
