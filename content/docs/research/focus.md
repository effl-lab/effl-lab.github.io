---
title: Focus
type: docs
bookToc: true
weight: 1
---

The long-term goal of EffL is to make AI more responsible--accessible, sustainable, and righteous.

As the first step, we are focusing on various facets of **Efficient ML**, which could help us make AI equally accessible to anybody on Earth, with no extreme carbon emission.

In particular, we work on three dimensions:

- **Inference.** We develop fast, low-resource methods to serve massive multimodal AI, e.g.,
	- [Model Compression](https://developer.apple.com/videos/play/wwdc2023/10047/)
	- [Parallel Decoding](https://pytorch.org/blog/flash-decoding/) 
	- [Batch Scheduling](https://www.anyscale.com/blog/continuous-batching-llm-inference)
- **Training.** We resolve the compute / memory bottlenecks for training large-scale models, e.g.,
	- [Meta-Learning](https://lilianweng.github.io/posts/2018-11-30-meta-learning/)
	- [Model Merging](https://gretel.ai/blog/what-is-model-soup)
	- [Parameter-Efficient Fine-Tuning](https://huggingface.co/blog/peft)
- **Data Dimension.** We design algorithms to handle data with extremely high dimensionality, e.g.,
	- [Data Compression](https://hific.github.io)
	- [High-res Video Processing](https://subin-kim-cv.github.io/NVP/)
	- [Time-Series Forecasting](https://github.com/ngruver/llmtime)

For more, see our [papers](/docs/research/papers).


### **Collaborators**
Our recent collaborators include (but are not limited to):
- [Samsung Advanced Institute of Technology](https://www.sait.samsung.co.kr/): ICML 2024, TMLR 2023
- [Google](https://research.google/): Visiting faculty researcher, Student intern
- [DeepMind](https://deepmind.google/): ICML 2023
- [Naver](https://navercorp.com/): NAACL 2024
- [University College London](https://www.ucl.ac.uk/): NeurIPS 2023, ICML 2023
- [University of Michigan](https://umich.edu/): CVPR 2024
- [Krafton](https://www.krafton.com/): CVPR 2024