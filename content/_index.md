---
title: EffL Lab
type: docs
bookToc: false
---

<style>
	.updown {
    	border: 10px solid white;
        width: 0.1px;
        height: 150px;
    }
</style>

`Make AI accessible to everyone with Efficient ML!`

Efficient Learning Lab, or simply {{< color "#455FAF" "EffL" >}}, is a research group led by [Jaeho Lee](https://jaeho-lee.github.io).  
We develop theories, algorithms, and systems to make ML more efficient.

To join us, apply us through [POSTECH EE](https://ee.postech.ac.kr) or [Graduate School of AI](https://ai.postech.ac.kr).  
For details, see [here](/docs/how-to-join/graduate/).

## **News**

{{< tabs "news" >}}
{{< tab "NAACL 2024" >}}

<div style= "float: left; margin-right: 20px; margin-bottom: 20px;">
{{< figure src="/images/news/naacl_ok_2024.png" alt="." width="250" height="100" >}} 
</div>
 
 

A paper on a **distillation-based method** to recognized named entities in multi-modal setup will be presented at **NAACL** **2024**! ðŸŽŠ

This is a joint work with **NAVER**.

(2024.03.14)

{{< /tab >}}

{{< tab "IEEE S&P" >}}
A paper on unlearning will be presented at IEEE S&P 2024.

(2024.03.10)
{{< /tab >}}
{{< tab "CVPR 2024" >}}
Two papers will be presented at CVPR 2024.
- An intriguing way to accelerate neural field training (led by Junwon and Sangyoon).
- A way to characterize visual biases in the form of text keywords (collaboration with UMich and KAIST).

(2024.02.27)
{{< /tab >}}

{{< tab "NeurIPS 2023" >}}
One conference paper and two workshop papers will be presented.
- Conference: Large-scale meta-learning for neural fields (with [Google DeepMind](https://www.deepmind.com)).
- Workshop: Model merging (led by Jiwoon) and Neural field training (led by Junwon and Sangyoon).

(2023.10.30)
{{< /tab >}}

{{< tab "@Google" >}}
Jaeho and Seungwoo started working at [Google](research.google).

They will work on LLM optimization as a visiting faculty and a student intern, respectively.

(2023.09.04)
{{< /tab >}}
{{< tab "ICML 2023" >}}
One conference paper and three workshop papers has been presented.  
- Conference: Data compression algorithm (with [Google DeepMind](https://www.deepmind.com)).  
- Workshop: Two on spurious correlations, and one on large-batch training.  

(2023.06)
{{< /tab >}}
{{< tab "TMLR" >}}
A paper on spurious correlation of conditional generative model has been published.

This is a joint work with [Samsung Advanced Institute of Technology](https://www.sait.samsung.co.kr/).  
(2023.06)
{{< /tab >}}

{{< /tabs>}}
